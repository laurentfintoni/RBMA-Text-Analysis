<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Part 6 Topic Modeling | Red Bull Music Academy Lectures: Text Analysis and Topic Modeling</title>
  <meta name="description" content="Exploring the RBMA lectures archive using text analysis and mining, a project for the Digital Text in the Humanities course at the university of Bologna, 2020/2021." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Part 6 Topic Modeling | Red Bull Music Academy Lectures: Text Analysis and Topic Modeling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/rbmacover.jpg" />
  <meta property="og:description" content="Exploring the RBMA lectures archive using text analysis and mining, a project for the Digital Text in the Humanities course at the university of Bologna, 2020/2021." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Part 6 Topic Modeling | Red Bull Music Academy Lectures: Text Analysis and Topic Modeling" />
  
  <meta name="twitter:description" content="Exploring the RBMA lectures archive using text analysis and mining, a project for the Digital Text in the Humanities course at the university of Bologna, 2020/2021." />
  <meta name="twitter:image" content="/images/rbmacover.jpg" />

<meta name="author" content="Laurent Fintoni" />


<meta name="date" content="2021-09-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exploratory-analysis-in-r.html"/>
<link rel="next" href="conclusion.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.19/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">RBMA Lectures: Text Analysis and Topic Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#pre-processing-steps"><i class="fa fa-check"></i><b>1.1</b> Pre-Processing Steps</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="corpus-cleaning-in-r.html"><a href="corpus-cleaning-in-r.html"><i class="fa fa-check"></i><b>2</b> Corpus Cleaning in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="corpus-cleaning-in-r.html"><a href="corpus-cleaning-in-r.html#basic-cleaning"><i class="fa fa-check"></i><b>2.1</b> Basic cleaning</a></li>
<li class="chapter" data-level="2.2" data-path="corpus-cleaning-in-r.html"><a href="corpus-cleaning-in-r.html#lemmatizing-vs-stemming"><i class="fa fa-check"></i><b>2.2</b> Lemmatizing vs Stemming</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="corpus-cleaning-in-python.html"><a href="corpus-cleaning-in-python.html"><i class="fa fa-check"></i><b>3</b> Corpus Cleaning in Python</a>
<ul>
<li class="chapter" data-level="3.1" data-path="corpus-cleaning-in-python.html"><a href="corpus-cleaning-in-python.html#basic-cleaning-pt-2"><i class="fa fa-check"></i><b>3.1</b> Basic cleaning pt 2</a></li>
<li class="chapter" data-level="3.2" data-path="corpus-cleaning-in-python.html"><a href="corpus-cleaning-in-python.html#lemmatizing-vs-stemming-pt-2"><i class="fa fa-check"></i><b>3.2</b> Lemmatizing vs Stemming pt 2</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory-analysis-in-python.html"><a href="exploratory-analysis-in-python.html"><i class="fa fa-check"></i><b>4</b> Exploratory Analysis in Python</a>
<ul>
<li class="chapter" data-level="4.1" data-path="exploratory-analysis-in-python.html"><a href="exploratory-analysis-in-python.html#pos-visualisation"><i class="fa fa-check"></i><b>4.1</b> POS Visualisation</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory-analysis-in-python.html"><a href="exploratory-analysis-in-python.html#lexical-diversity"><i class="fa fa-check"></i><b>4.2</b> Lexical diversity</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory-analysis-in-python.html"><a href="exploratory-analysis-in-python.html#most-common-words"><i class="fa fa-check"></i><b>4.3</b> Most Common Words</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="exploratory-analysis-in-r.html"><a href="exploratory-analysis-in-r.html"><i class="fa fa-check"></i><b>5</b> Exploratory Analysis in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="exploratory-analysis-in-r.html"><a href="exploratory-analysis-in-r.html#co-occurences"><i class="fa fa-check"></i><b>5.1</b> Co-occurences</a></li>
<li class="chapter" data-level="5.2" data-path="exploratory-analysis-in-r.html"><a href="exploratory-analysis-in-r.html#term-frequency-vs-inverse-document-frequency"><i class="fa fa-check"></i><b>5.2</b> Term Frequency vs Inverse Document Frequency</a></li>
<li class="chapter" data-level="5.3" data-path="exploratory-analysis-in-r.html"><a href="exploratory-analysis-in-r.html#collocations"><i class="fa fa-check"></i><b>5.3</b> Collocations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="topic-modeling.html"><a href="topic-modeling.html"><i class="fa fa-check"></i><b>6</b> Topic Modeling</a>
<ul>
<li class="chapter" data-level="6.1" data-path="topic-modeling.html"><a href="topic-modeling.html#topic-modeling-in-r"><i class="fa fa-check"></i><b>6.1</b> Topic Modeling in R</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="topic-modeling.html"><a href="topic-modeling.html#topic-modeling-with-topicmodels"><i class="fa fa-check"></i><b>6.1.1</b> Topic Modeling with topicmodels</a></li>
<li class="chapter" data-level="6.1.2" data-path="topic-modeling.html"><a href="topic-modeling.html#topic-modeling-with-stm"><i class="fa fa-check"></i><b>6.1.2</b> Topic Modeling with stm</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="topic-modeling.html"><a href="topic-modeling.html#topic-modeling-in-python"><i class="fa fa-check"></i><b>6.2</b> Topic Modeling in Python</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>7</b> Conclusion</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Red Bull Music Academy Lectures: Text Analysis and Topic Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="topic-modeling" class="section level1" number="6">
<h1><span class="header-section-number">Part 6</span> Topic Modeling</h1>
<p>For the last step of this project I wanted to attempt some basic text mining in the form of topic modeling, to see if the results would align with the analysis and my knowledge of the corpus. As with the rest of the project I decided to undertake this in both R and Python for learning and comparison purposes.</p>
<p>In R I used the stm <span class="citation">(<a href="#ref-R-stm" role="doc-biblioref">Roberts, Stewart, and Tingley 2020</a>)</span> and topicmodels <span class="citation">(<a href="#ref-R-topicmodels" role="doc-biblioref">Grün and Hornik 2021</a>)</span> packages. In Python I used the MALLET Java package<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> via the Little Mallet Wrapper library<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>, based on Melanie Walsh’s tutorial in <em>Introduction to Cultural Analytics &amp; Python</em><a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>). In every case, topic modeling was done using the Latent Dirichlet Analysis (LDA) algorithm.</p>
<p>I also spent some time looking into the Latent Semantic Analysis (LSA) algorithm, which is possible using stm, but this proved somewhat more complicated than I had time for. With Python, I also did some tests using the gensim library<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> but the tuning required to achieve useful results was more involving than MALLET.</p>
<p>I ran the topic models on both cleaned versions as the raw version is too noisy to be meaningful.</p>
<hr />
<div id="topic-modeling-in-r" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Topic Modeling in R</h2>
<p>To start with I created Document-Feature Matrixes from the quanteda versions of the corpus, trimming out words that appear less than 7.5% and more than 90% of the time. In my first attempts I’d set the trim theshold to ignore term frequency below 2000 but that proved too drastic resulting in topics that were a lot like the main themes I discovered in the exploratory analsysis.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="topic-modeling.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb103-2"><a href="topic-modeling.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb103-3"><a href="topic-modeling.html#cb103-3" aria-hidden="true" tabindex="-1"></a>r_corpus<span class="ot">&lt;-</span> <span class="fu">VCorpus</span>(<span class="fu">DirSource</span>(<span class="st">&quot;FILES/RBMA_CLEAN_R_LEMM_V2&quot;</span>)); py_corpus<span class="ot">&lt;-</span> <span class="fu">VCorpus</span>(<span class="fu">DirSource</span>(<span class="st">&quot;FILES/RBMA_CLEAN_PY_LEMM_V2_POS&quot;</span>)); r_corpus_q<span class="ot">&lt;-</span> <span class="fu">corpus</span>(r_corpus); py_corpus_q<span class="ot">&lt;-</span> <span class="fu">corpus</span>(py_corpus); <span class="fu">docnames</span>(r_corpus_q)<span class="ot">&lt;-</span> <span class="fu">docvars</span>(r_corpus_q, <span class="st">&quot;id&quot;</span>); <span class="fu">docnames</span>(py_corpus_q)<span class="ot">&lt;-</span> <span class="fu">docvars</span>(py_corpus_q, <span class="st">&quot;id&quot;</span>)</span>
<span id="cb103-4"><a href="topic-modeling.html#cb103-4" aria-hidden="true" tabindex="-1"></a>r_dfm<span class="ot">&lt;-</span> <span class="fu">tokens</span>(r_corpus_q); r_dfm<span class="ot">&lt;-</span> <span class="fu">dfm</span>(r_dfm); py_dfm<span class="ot">&lt;-</span> <span class="fu">tokens</span>(py_corpus_q); py_dfm<span class="ot">&lt;-</span> <span class="fu">dfm</span>(py_dfm)</span>
<span id="cb103-5"><a href="topic-modeling.html#cb103-5" aria-hidden="true" tabindex="-1"></a>r_dfm<span class="ot">&lt;-</span> <span class="fu">dfm_trim</span>(r_dfm, <span class="at">min_docfreq =</span> <span class="fl">0.075</span>, <span class="at">max_docfreq =</span> <span class="fl">0.90</span>, <span class="at">docfreq_type =</span> <span class="st">&quot;prop&quot;</span>); py_dfm<span class="ot">&lt;-</span> <span class="fu">dfm_trim</span>(py_dfm, <span class="at">min_docfreq =</span> <span class="fl">0.075</span>, <span class="at">max_docfreq =</span> <span class="fl">0.90</span>, <span class="at">docfreq_type =</span> <span class="st">&quot;prop&quot;</span>)</span>
<span id="cb103-6"><a href="topic-modeling.html#cb103-6" aria-hidden="true" tabindex="-1"></a>r_dfm; py_dfm</span></code></pre></div>
<pre><code>## Document-feature matrix of: 468 documents, 2,700 features (73.89% sparse) and 7 docvars.
##                          features
## docs                      hello turn lecture present job easy run spend hour
##   a-guy-called-gerald.txt     2    1       2       1   2    4   5     1    3
##   addison-groove.txt          4    6       2       0   4    0   0     6    8
##   aisha-devi.txt              1    1       1       0   0    1   1     1    6
##   alec-empire.txt             1    4       0       2   2    4   3     0    7
##   alex-barck.txt              0    1       0       0   0   11   4     0    1
##   alex-rosner.txt             0    2       2       0   4    3   0     0    2
##                          features
## docs                      sheet
##   a-guy-called-gerald.txt     1
##   addison-groove.txt          0
##   aisha-devi.txt              0
##   alec-empire.txt             0
##   alex-barck.txt              0
##   alex-rosner.txt             0
## [ reached max_ndoc ... 462 more documents, reached max_nfeat ... 2,690 more features ]</code></pre>
<pre><code>## Document-feature matrix of: 468 documents, 2,858 features (74.00% sparse) and 7 docvars.
##                          features
## docs                      hello thank turn lecture present job easy ran spent
##   a-guy-called-gerald.txt     2     5    1       2       1   2    4   1     1
##   addison-groove.txt          1     2    6       2       0   4    0   0     2
##   aisha-devi.txt              0     5    1       1       0   0    1   0     1
##   alec-empire.txt             0     0    4       0       2   2    4   1     0
##   alex-barck.txt              0     4    1       0       0   0   11   0     0
##   alex-rosner.txt             0     0    2       2       0   4    3   0     0
##                          features
## docs                      hour
##   a-guy-called-gerald.txt    3
##   addison-groove.txt         8
##   aisha-devi.txt             6
##   alec-empire.txt            7
##   alex-barck.txt             1
##   alex-rosner.txt            2
## [ reached max_ndoc ... 462 more documents, reached max_nfeat ... 2,848 more features ]</code></pre>
<div id="topic-modeling-with-topicmodels" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Topic Modeling with topicmodels</h3>
<p>Next I set a topic count of 15, converted the DFM to the topicmodels format and then used that to calculate the LDA models.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="topic-modeling.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb106-2"><a href="topic-modeling.html#cb106-2" aria-hidden="true" tabindex="-1"></a>topic.count <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb106-3"><a href="topic-modeling.html#cb106-3" aria-hidden="true" tabindex="-1"></a>dfm2topicmodels <span class="ot">&lt;-</span> <span class="fu">convert</span>(r_dfm, <span class="at">to =</span> <span class="st">&quot;topicmodels&quot;</span>)</span>
<span id="cb106-4"><a href="topic-modeling.html#cb106-4" aria-hidden="true" tabindex="-1"></a>lda.model <span class="ot">&lt;-</span> <span class="fu">LDA</span>(dfm2topicmodels, topic.count)</span></code></pre></div>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="topic-modeling.html#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co">#same with python corpus</span></span>
<span id="cb107-2"><a href="topic-modeling.html#cb107-2" aria-hidden="true" tabindex="-1"></a>dfm2topicmodels_py<span class="ot">&lt;-</span> <span class="fu">convert</span>(py_dfm, <span class="at">to =</span> <span class="st">&quot;topicmodels&quot;</span>)</span>
<span id="cb107-3"><a href="topic-modeling.html#cb107-3" aria-hidden="true" tabindex="-1"></a>lda.model_py<span class="ot">&lt;-</span> <span class="fu">LDA</span>(dfm2topicmodels_py, topic.count)</span></code></pre></div>
<p>I then displayed the resulting topics in a dataframe with the top 10 terms from each for ease of analysis.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="topic-modeling.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.frame</span>(<span class="fu">terms</span>(lda.model, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>##     Topic 1 Topic 2   Topic 3    Topic 4   Topic 5    Topic 6 Topic 7   Topic 8
## 1  musician    club everybody       beat     label     create      dj       man
## 2      sing      dj     gonna       drum      tune        art   radio      shit
## 3      jazz   house       man     sample     quite experience  hiphop      beat
## 4   country   night       kid       cool      bass      write    club       rap
## 5      band   dance     cause     hiphop    london     person    york        yo
## 6     young   party      cool         dj   release     moment    soul      fuck
## 7       man detroit     story     pretty        uk    process   disco everybody
## 8     dance  techno    school      house obviously      sense    tape    hiphop
## 9   african     mix     group definitely   dubstep    certain     buy     crazy
## 10    black   scene     black      dance      drum      quite    sell      dude
##     Topic 9    Topic 10 Topic 11  Topic 12 Topic 13 Topic 14   Topic 15
## 1      band synthesizer     room      sort   course      mix      piece
## 2      sing     machine    write      band      yes   master       film
## 3      drum  electronic      sit     label    label engineer      movie
## 4    guitar  instrument producer      fuck  release      cut      write
## 5     write    computer    amaze    pretty   berlin    vinyl instrument
## 6  musician       build     sing     money     band     room      piano
## 7    player        note      hit      sell  example     bass        yes
## 8      bass     control     turn      film   techno     tape       game
## 9   drummer   basically    vocal basically   french       cd   composer
## 10    group        tape      god     scene    quite  process      score</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="topic-modeling.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.frame</span>(<span class="fu">terms</span>(lda.model_py, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>##        Topic 1    Topic 2  Topic 3  Topic 4 Topic 5    Topic 6 Topic 7
## 1  synthesizer        hop      mix musician      dj     person    bass
## 2   instrument       beat   master      yes    club      woman    tune
## 3      machine        hip engineer     sing   dance      thank      dj
## 4     computer     sample     room     band   house      write   quite
## 5         note       cool      cut     jazz   party        art  london
## 6         drum        rap     tape  country   disco     moment    drum
## 7   electronic definitely     bass american   night      voice    sort
## 8      control     pretty    vinyl    dance     mix       word      uk
## 9       course   producer       cd    group   scene understand    club
## 10    keyboard       sort     high    young  techno       sing   scene
##      Topic 8 Topic 9   Topic 10  Topic 11 Topic 12  Topic 13 Topic 14 Topic 15
## 1        hip    film      piece       man    label      band    write    cause
## 2        hop   movie       sort      shit  release       man    piano     drum
## 3         dj    band     create      beat     sell  musician   string       dj
## 4  everybody   video      quite      fuck    money      drum     band     hour
## 5        rap    sort interested everybody   techno      soul     drum    laugh
## 6     sample   scene      guess        yo       dj    guitar     sing    gonna
## 7      radio  course      space       rap  certain     group     game  machine
## 8      money   write experience     crazy    quite everybody producer  detroit
## 9       beat   later instrument       kid  company     james   melody   moment
## 10     gonna   story      sense  somebody   berlin      knew   player     beat</code></pre>
<p>We see some expected clustering:</p>
<ul>
<li>Topics 4 (R) and 7 (Python) clearly refer to mastering / engineering lectures.</li>
<li>5 and 9 (R) and 14 and 15 (Python) appear to lean heavily towards DJ culture, with the Python topics making more of historically important locations such as Chicago, Detroit, (New) York and London.</li>
<li>8 (R) and 6 (Python) instead hint at more traditional forms of music creation (band, instruments, writing, singing).</li>
<li>11 (R) and 3 (Python) seem to capture some of the hip-hop related vocabulary I mentioned in <a href="exploratory-analysis-in-python.html#lexical-diversity">4.2</a> but 2 and 15 (R) and 12 (Python) do a better job at summing up hip-hop practices.</li>
<li>14 (R) and 8 (Python) capture the essential connection between Berlin and Detroit within techno music.</li>
<li>7 (R) and 9 (Python) both seem to reflect on music and moving images with “film,” “video,”, and “movie,” which might be due to the presence in the corpus of lectures with movie directors and the increasingly prevalent industry practice of syncing music to images which became a more regular topic of conversation in the second decade of lectures.</li>
<li>3 (R) and 1 (Python) reflect electronic music practices and instruments.</li>
<li>13 (R) and 5 (Python) clearly seem to point towards the business sub-theme of the corpus, with references to “label”, “buy/sell,” and “release.”</li>
</ul>
<p>Another way to view similarities between topics within the same model is to plot them using a dendogram, which shows clusters of similar topics as well as their distances/dissimilarity.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="topic-modeling.html#cb112-1" aria-hidden="true" tabindex="-1"></a>lda.similarity <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(lda.model<span class="sc">@</span>beta) <span class="sc">%&gt;%</span></span>
<span id="cb112-2"><a href="topic-modeling.html#cb112-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale</span>() <span class="sc">%&gt;%</span></span>
<span id="cb112-3"><a href="topic-modeling.html#cb112-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dist</span>(<span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb112-4"><a href="topic-modeling.html#cb112-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hclust</span>(<span class="at">method =</span> <span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb112-5"><a href="topic-modeling.html#cb112-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-6"><a href="topic-modeling.html#cb112-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>))</span>
<span id="cb112-7"><a href="topic-modeling.html#cb112-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda.similarity,</span>
<span id="cb112-8"><a href="topic-modeling.html#cb112-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;LDA topic similarity by features&quot;</span>,</span>
<span id="cb112-9"><a href="topic-modeling.html#cb112-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb112-10"><a href="topic-modeling.html#cb112-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">sub =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/LDA%20Similarity-1.png" width="672" /></p>
<p>In the R version, we can see that topics 3 (electronic music) and 5 (DJ culture) and 13 (business) appear most distant from the rest and that there are two major clusters: in the case of topics 1, 9, 11, 2, and 15 it seems logical to assume that this is due to how they relate to hip-hop and club cultures, while the second cluster is more diverse though within that topics 7, 10, 12, and 14 seem to be closely related via more abstract themes such as art, technology, and experimentation.</p>
<p>In the Python version, we again see the electronic music topic (1) as most distant alongside two of the hip-hop topics (2, 3), though this distance is less pronounced, and the two major clusters appear to have similar internal relations, with DJing, hip-hop, writing, and production on the left and mastering, electronic music, clubs, and machines on the right.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="topic-modeling.html#cb113-1" aria-hidden="true" tabindex="-1"></a>lda.similarity <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(lda.model_py<span class="sc">@</span>beta) <span class="sc">%&gt;%</span></span>
<span id="cb113-2"><a href="topic-modeling.html#cb113-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale</span>() <span class="sc">%&gt;%</span></span>
<span id="cb113-3"><a href="topic-modeling.html#cb113-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dist</span>(<span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb113-4"><a href="topic-modeling.html#cb113-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hclust</span>(<span class="at">method =</span> <span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb113-5"><a href="topic-modeling.html#cb113-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-6"><a href="topic-modeling.html#cb113-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>))</span>
<span id="cb113-7"><a href="topic-modeling.html#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda.similarity,</span>
<span id="cb113-8"><a href="topic-modeling.html#cb113-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;LDA topic similarity by features&quot;</span>,</span>
<span id="cb113-9"><a href="topic-modeling.html#cb113-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb113-10"><a href="topic-modeling.html#cb113-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">sub =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/LDA%20Similarity%20Python-1.png" width="672" /></p>
<p>Lastly we can also view the most likely topics for each document using the get_topics method from topicmodels. For example if we check which documents are likely associated with topic 3 in R (electronic music) we see lectures with pioneers of synthesis such as Bob Moog, Don Buchla, Suzanne Ciani, and Pauline Oliveros as well as Orchestra di Santa Cecilia, most likely due to the topic also including terms such as “note,” “piano,” and “instrument.” Similar results can be obtained for other well defined topics (both in R and Python) but I was also curious to look into topics that are harder to define, such as topic 12 (R) and 13 (Python) which seem to point towards abstract ideas of experimentation. In both cases we get quite a lot of results (25 in R, 44 in Python) across a range of lectures that do seem to confirm experimentation as perhaps the underlying idea of the topic. Below is the code I used to get the most likely topics and filter them for easy analysis.</p>
<pre><code>topics_by_doc&lt;- get_topics(lda.model)
topics_by_doc&lt;- t(topics_by_doc)
topics_by_doc_df&lt;- as.data.frame(topics_by_doc)
topics_count&lt;- colCounts(topics_by_doc, value = 12, useNames = TRUE, na.rm = TRUE)
topics_count&lt;- as.data.frame(topics_count)
topics_count_2&lt;- subset(topics_count, topics_count &gt; 0)</code></pre>
</div>
<div id="topic-modeling-with-stm" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Topic Modeling with stm</h3>
<p>The stm package is similar to topicmodels but allows for more diagnosis and fine tuning, including the estimation and evaluation of potential models ahead of actually working with them.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a> In the context of this project, stm was the first topic modeling package I ran tests with and it offers a nice range of quick vizualisations.</p>
<p>As with topicmodels, I first converted the dfm to the stm format before calculating the models.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="topic-modeling.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stm)</span>
<span id="cb115-2"><a href="topic-modeling.html#cb115-2" aria-hidden="true" tabindex="-1"></a>dfm2stm <span class="ot">&lt;-</span> <span class="fu">convert</span>(r_dfm, <span class="at">to =</span> <span class="st">&quot;stm&quot;</span>)</span>
<span id="cb115-3"><a href="topic-modeling.html#cb115-3" aria-hidden="true" tabindex="-1"></a>model.stm <span class="ot">&lt;-</span> <span class="fu">stm</span>(dfm2stm<span class="sc">$</span>documents, dfm2stm<span class="sc">$</span>vocab, <span class="at">K =</span> topic.count, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="topic-modeling.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="co">#same with python corpus</span></span>
<span id="cb116-2"><a href="topic-modeling.html#cb116-2" aria-hidden="true" tabindex="-1"></a>dfm2stm_py <span class="ot">&lt;-</span> <span class="fu">convert</span>(py_dfm, <span class="at">to =</span> <span class="st">&quot;stm&quot;</span>)</span>
<span id="cb116-3"><a href="topic-modeling.html#cb116-3" aria-hidden="true" tabindex="-1"></a>model.stm_py <span class="ot">&lt;-</span> <span class="fu">stm</span>(dfm2stm_py<span class="sc">$</span>documents, dfm2stm_py<span class="sc">$</span>vocab, <span class="at">K =</span> topic.count, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Again, we can first view the resulting topics as a data frame. Overall the topics appear to be similar to those obtained with topicmodels but with variations in which terms make the top 10 (topic 1 in Python is the most obviously different between the two package results).</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="topic-modeling.html#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">labelTopics</span>(model.stm, <span class="at">n =</span> <span class="dv">10</span>)<span class="sc">$</span>prob))</span></code></pre></div>
<pre><code>##         V1        V2        V3          V4         V5      V6         V7     V8
## 1   techno    hiphop       man synthesizer       beat   label       sing   bass
## 2   berlin    sample      shit     machine       game    sell      speak   tune
## 3   course      beat      fuck       build   producer   money      black   drum
## 4  detroit      drum everybody  electronic        rap release    country london
## 5     club        dj     gonna  instrument       send     buy   musician  quite
## 6    label       rap        yo    computer       cool     pay      young     dj
## 7      yes everybody       kid        note definitely    sign understand     uk
## 8       dj      cool       rap     control     sample company      woman   club
## 9  germany    pretty         y   basically     create   remix        art  scene
## 10  german     crazy     crazy     company     hiphop   quite     person   sort
##          V9       V10      V11     V12        V13   V14       V15
## 1       mix      film     band  robert      piece  club      sort
## 2    master     movie     sing    york instrument    dj      band
## 3  engineer     video    write   disco        yes dance     guess
## 4      room      sort     drum   house      write house     write
## 5       cut     scene   guitar    club      piano party    pretty
## 6     vinyl     story   player    tony   musician night     quite
## 7      tape      clip musician   group     create   mix obviously
## 8      bass     write     bass detroit     course djing      cool
## 9      loud     shoot      hit   radio    example scene   process
## 10  process basically producer   light      voice disco      tour</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="topic-modeling.html#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">labelTopics</span>(model.stm_py, <span class="at">n =</span> <span class="dv">10</span>)<span class="sc">$</span>prob))</span></code></pre></div>
<pre><code>##            V1        V2        V3          V4       V5         V6     V7
## 1       thank       hip       man synthesizer    label       beat   tune
## 2      person       hop      shit     machine  release       game   bass
## 3         art    sample      fuck    computer     sell   producer     dj
## 4       write        dj everybody  instrument    money     sample   drum
## 5       woman      beat        yo     control   techno definitely london
## 6       voice       rap     gonna        note       dj       cool  quite
## 7      moment      drum       rap     company  company     create     uk
## 8        sing      cool       kid   basically     club      video   club
## 9        word everybody     money  electronic    quite        rap  scene
## 10 understand    pretty      knew      design business     pretty   sort
##          V8     V9      V10      V11       V12        V13    V14     V15
## 1      band   film    write      mix      band      piece     dj      dj
## 2  musician  movie     drum   master      sort instrument   club detroit
## 3      jazz  video     band engineer    pretty        yes  dance   house
## 4   country   sort     sing     room     quite      piano  disco    club
## 5     young course      hit      cut     guess      write  night chicago
## 6     black  scene   player     tape     write   musician  party    city
## 7     group  story   guitar    vinyl      tour     course  house  techno
## 8   african   clip producer     bass obviously    example  scene   dance
## 9      sing  later      man       cd       pop     create    mix   party
## 10   africa  write     bass     high     label  classical french   night</code></pre>
<p>The summary plot function in stm makes it much easier to see how prevalent each topic is across the corpus and its overall theme. In both versions, the three most prevalent topics are all related to writing, singing, and instruments while hip-hop, DJing, and electronic music occupy the middle range and the mastering/engineering, synthesizer building, and film topics in the bottom 5.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="topic-modeling.html#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model.stm, <span class="at">type =</span> <span class="st">&quot;summary&quot;</span>, <span class="at">text.cex =</span> <span class="fl">0.7</span>, <span class="at">main =</span> <span class="st">&quot;STM topic shares for R version&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Share estimation&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/STM%20Topic%20Shares-1.png" width="672" /></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="topic-modeling.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model.stm_py, <span class="at">type =</span> <span class="st">&quot;summary&quot;</span>, <span class="at">text.cex =</span> <span class="fl">0.7</span>, <span class="at">main =</span> <span class="st">&quot;STM topic shares for Python version&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Share estimation&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/STM%20Topic%20Shares-2.png" width="672" /></p>
<p>There is also a word cloud function in stm, which is handy for delving deeper into the topics, as well as a perspective version of the plot which allows us to see similarities and differences between topics. For example, looking at the topics from the R version for hip-hop (2) and electronic music (4) we can see that “drum,” “keyboard,” “break,” and “money” are all pretty central to both topics while “sample” and “synthesizers,” as well as the genre names of “hip-hop” and “electronic” are more closely aligned with each respective topic. At a first glance this seems to align with how people talk about and practice both genres.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="topic-modeling.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model.stm, <span class="at">type =</span> <span class="st">&quot;perspectives&quot;</span>, <span class="at">n =</span> <span class="dv">30</span>, <span class="at">topics =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>), <span class="at">main =</span> <span class="st">&quot;Putting two different topics in perspective&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/STM%20Perspectives-1.png" width="672" /></p>
<hr />
</div>
</div>
<div id="topic-modeling-in-python" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Topic Modeling in Python</h2>
<p>For topic modelling in Python I made use of the pickled dataframes created during the exploratory analysis. Unlike in R, the little mallet wrapper works directly with the texts rather than DFMs.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="topic-modeling.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reticulate)</span></code></pre></div>
<div class="sourceCode" id="cb125"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="topic-modeling.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code></pre></div>
<div class="sourceCode" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="topic-modeling.html#cb126-1" aria-hidden="true" tabindex="-1"></a>rbma_corpus_clean_lemm_v2_pickle <span class="op">=</span> <span class="st">&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA R V2 LEMM.pkl&#39;</span></span>
<span id="cb126-2"><a href="topic-modeling.html#cb126-2" aria-hidden="true" tabindex="-1"></a>rbma_R_df <span class="op">=</span> pd.read_pickle(rbma_corpus_clean_lemm_v2_pickle)</span>
<span id="cb126-3"><a href="topic-modeling.html#cb126-3" aria-hidden="true" tabindex="-1"></a>rbma_corpus_py_clean_lemm_v2_pickle <span class="op">=</span> <span class="st">&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA PY V2 LEMM.pkl&#39;</span></span>
<span id="cb126-4"><a href="topic-modeling.html#cb126-4" aria-hidden="true" tabindex="-1"></a>rbma_python_df <span class="op">=</span> pd.read_pickle(rbma_corpus_py_clean_lemm_v2_pickle)</span></code></pre></div>
<div class="sourceCode" id="cb127"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb127-1"><a href="topic-modeling.html#cb127-1" aria-hidden="true" tabindex="-1"></a>path_to_mallet <span class="op">=</span> <span class="st">&#39;/Users/laurentfintoni/mallet-2.0.8/bin/mallet&#39;</span></span>
<span id="cb127-2"><a href="topic-modeling.html#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> little_mallet_wrapper</span>
<span id="cb127-3"><a href="topic-modeling.html#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn</span>
<span id="cb127-4"><a href="topic-modeling.html#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span></code></pre></div>
<p>We check the statistics for our dataset, which align with what we saw before.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="topic-modeling.html#cb128-1" aria-hidden="true" tabindex="-1"></a>little_mallet_wrapper.print_dataset_stats(rbma_R_df.Transcript)</span></code></pre></div>
<pre><code>## Number of Documents: 468
## Mean Number of Words per Document: 4195.0
## Vocabulary Size: 45354</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="topic-modeling.html#cb130-1" aria-hidden="true" tabindex="-1"></a>little_mallet_wrapper.print_dataset_stats(rbma_python_df.Transcript)</span></code></pre></div>
<pre><code>## Number of Documents: 468
## Mean Number of Words per Document: 4803.6
## Vocabulary Size: 37950</code></pre>
<p>We then set the number topics to 15 and created variables for the two training sets and their respective output paths.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="topic-modeling.html#cb132-1" aria-hidden="true" tabindex="-1"></a>num_topics <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb132-2"><a href="topic-modeling.html#cb132-2" aria-hidden="true" tabindex="-1"></a>training_data <span class="op">=</span> rbma_R_df.Transcript</span>
<span id="cb132-3"><a href="topic-modeling.html#cb132-3" aria-hidden="true" tabindex="-1"></a>training_data_py <span class="op">=</span> rbma_python_df.Transcript</span>
<span id="cb132-4"><a href="topic-modeling.html#cb132-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-5"><a href="topic-modeling.html#cb132-5" aria-hidden="true" tabindex="-1"></a>output_directory_path <span class="op">=</span> <span class="st">&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/Topic Model Output/RBMA R FINAL&#39;</span></span>
<span id="cb132-6"><a href="topic-modeling.html#cb132-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-7"><a href="topic-modeling.html#cb132-7" aria-hidden="true" tabindex="-1"></a>Path(<span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path<span class="sc">}</span><span class="ss">&quot;</span>).mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb132-8"><a href="topic-modeling.html#cb132-8" aria-hidden="true" tabindex="-1"></a>path_to_training_data           <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path<span class="sc">}</span><span class="ss">/training.txt&quot;</span></span>
<span id="cb132-9"><a href="topic-modeling.html#cb132-9" aria-hidden="true" tabindex="-1"></a>path_to_formatted_training_data <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path<span class="sc">}</span><span class="ss">/mallet.training&quot;</span></span>
<span id="cb132-10"><a href="topic-modeling.html#cb132-10" aria-hidden="true" tabindex="-1"></a>path_to_model                   <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path<span class="sc">}</span><span class="ss">/mallet.model.</span><span class="sc">{</span><span class="bu">str</span>(num_topics)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb132-11"><a href="topic-modeling.html#cb132-11" aria-hidden="true" tabindex="-1"></a>path_to_topic_keys              <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path<span class="sc">}</span><span class="ss">/mallet.topic_keys.</span><span class="sc">{</span><span class="bu">str</span>(num_topics)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb132-12"><a href="topic-modeling.html#cb132-12" aria-hidden="true" tabindex="-1"></a>path_to_topic_distributions     <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path<span class="sc">}</span><span class="ss">/mallet.topic_distributions.</span><span class="sc">{</span><span class="bu">str</span>(num_topics)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb132-13"><a href="topic-modeling.html#cb132-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-14"><a href="topic-modeling.html#cb132-14" aria-hidden="true" tabindex="-1"></a><span class="co">#same for python version</span></span>
<span id="cb132-15"><a href="topic-modeling.html#cb132-15" aria-hidden="true" tabindex="-1"></a>output_directory_path_py <span class="op">=</span> <span class="st">&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/Topic Model Output/RBMA Python FINAL&#39;</span></span>
<span id="cb132-16"><a href="topic-modeling.html#cb132-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-17"><a href="topic-modeling.html#cb132-17" aria-hidden="true" tabindex="-1"></a>Path(<span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path_py<span class="sc">}</span><span class="ss">&quot;</span>).mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb132-18"><a href="topic-modeling.html#cb132-18" aria-hidden="true" tabindex="-1"></a>path_to_training_data_py           <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path_py<span class="sc">}</span><span class="ss">/training.txt&quot;</span></span>
<span id="cb132-19"><a href="topic-modeling.html#cb132-19" aria-hidden="true" tabindex="-1"></a>path_to_formatted_training_data_py <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path_py<span class="sc">}</span><span class="ss">/mallet.training&quot;</span></span>
<span id="cb132-20"><a href="topic-modeling.html#cb132-20" aria-hidden="true" tabindex="-1"></a>path_to_model_py                   <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path_py<span class="sc">}</span><span class="ss">/mallet.model.</span><span class="sc">{</span><span class="bu">str</span>(num_topics)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb132-21"><a href="topic-modeling.html#cb132-21" aria-hidden="true" tabindex="-1"></a>path_to_topic_keys_py              <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path_py<span class="sc">}</span><span class="ss">/mallet.topic_keys.</span><span class="sc">{</span><span class="bu">str</span>(num_topics)<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb132-22"><a href="topic-modeling.html#cb132-22" aria-hidden="true" tabindex="-1"></a>path_to_topic_distributions_py     <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>output_directory_path_py<span class="sc">}</span><span class="ss">/mallet.topic_distributions.</span><span class="sc">{</span><span class="bu">str</span>(num_topics)<span class="sc">}</span><span class="ss">&quot;</span></span></code></pre></div>
<p>Finally we train the models and export the data to be used for analysis in the relevant folders.</p>
<pre><code>little_mallet_wrapper.quick_train_topic_model(path_to_mallet, output_directory_path, num_topics, training_data)</code></pre>
<p>We can then take the results and put them in a dataframe to quickly see how they differ from the R packages.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="topic-modeling.html#cb134-1" aria-hidden="true" tabindex="-1"></a>topics_r <span class="op">=</span> little_mallet_wrapper.load_topic_keys(path_to_topic_keys)</span>
<span id="cb134-2"><a href="topic-modeling.html#cb134-2" aria-hidden="true" tabindex="-1"></a>topics_py <span class="op">=</span> little_mallet_wrapper.load_topic_keys(path_to_topic_keys_py)</span></code></pre></div>
<pre><code>topics_df = pd.DataFrame([i for i in topics_r])
topics_df = topics_df.transpose()
topics_df
topics_df_py = pd.DataFrame([i for i in topics_py])
topics_df_py = topics_df_py.transpose()
topics_df_py</code></pre>
<p><img src="images/topicmodels_r.png" width="100%" /><img src="images/topicmodels_py.png" width="100%" /></p>
<p>Overall the topics appear to be pretty similar to what the R packages returned though by working on the entire corpus rather than the matrixes we see many of the previously identified frequent words - “music,” “say,” and “like” - creeping up across all topics. As a result it’s a little harder to define these topics, though having more than 10 words for each to look at helps compensate. For example in the Python version of the corpus, it looks like the mastering/engineering topic has been split across topics 0 and 1 while in the R version (topic 14) it’s most similar to what modeling in R produced.</p>
<p>I also took a look at probable topic distribution for specific lectures.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb136-1"><a href="topic-modeling.html#cb136-1" aria-hidden="true" tabindex="-1"></a>topic_distributions_r <span class="op">=</span> little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)</span>
<span id="cb136-2"><a href="topic-modeling.html#cb136-2" aria-hidden="true" tabindex="-1"></a>topic_distributions_py <span class="op">=</span> little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions_py)</span></code></pre></div>
<div class="sourceCode" id="cb137"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="topic-modeling.html#cb137-1" aria-hidden="true" tabindex="-1"></a>lecture_titles_r <span class="op">=</span> rbma_R_df.index.tolist()</span>
<span id="cb137-2"><a href="topic-modeling.html#cb137-2" aria-hidden="true" tabindex="-1"></a>lecture_to_check_r <span class="op">=</span> <span class="st">&quot;young guru &quot;</span></span>
<span id="cb137-3"><a href="topic-modeling.html#cb137-3" aria-hidden="true" tabindex="-1"></a>lecture_number_r <span class="op">=</span> lecture_titles_r.index(lecture_to_check_r)</span>
<span id="cb137-4"><a href="topic-modeling.html#cb137-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> topic_dist_r():</span>
<span id="cb137-5"><a href="topic-modeling.html#cb137-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Topic Distributions for </span><span class="sc">{</span>lecture_titles_r[lecture_number_r]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb137-6"><a href="topic-modeling.html#cb137-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> topic_number, (topic, topic_distribution) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(topics_r, topic_distributions_r[lecture_number_r])):</span>
<span id="cb137-7"><a href="topic-modeling.html#cb137-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Topic </span><span class="sc">{</span>topic_number<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>topic[:<span class="dv">6</span>]<span class="sc">}</span><span class="ss"> Probability: </span><span class="sc">{</span><span class="bu">round</span>(topic_distribution, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb137-8"><a href="topic-modeling.html#cb137-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-9"><a href="topic-modeling.html#cb137-9" aria-hidden="true" tabindex="-1"></a>topic_dist_r()</span></code></pre></div>
<pre><code>## Topic Distributions for young guru 
## Topic 0 [&#39;new&#39;, &#39;play&#39;, &#39;club&#39;, &#39;york&#39;, &#39;people&#39;, &#39;come&#39;] Probability: 0.031
## Topic 1 [&#39;people&#39;, &#39;black&#39;, &#39;come&#39;, &#39;world&#39;, &#39;country&#39;, &#39;say&#39;] Probability: 0.019
## Topic 2 [&#39;film&#39;, &#39;movie&#39;, &#39;music&#39;, &#39;video&#39;, &#39;see&#39;, &#39;game&#39;] Probability: 0.011
## Topic 3 [&#39;call&#39;, &#39;record&#39;, &#39;guy&#39;, &#39;say&#39;, &#39;band&#39;, &#39;name&#39;] Probability: 0.021
## Topic 4 [&#39;song&#39;, &#39;track&#39;, &#39;record&#39;, &#39;album&#39;, &#39;play&#39;, &#39;make&#39;] Probability: 0.046
## Topic 5 [&#39;sound&#39;, &#39;use&#39;, &#39;drum&#39;, &#39;play&#39;, &#39;make&#39;, &#39;bass&#39;] Probability: 0.025
## Topic 6 [&#39;sound&#39;, &#39;music&#39;, &#39;way&#39;, &#39;different&#39;, &#39;think&#39;, &#39;also&#39;] Probability: 0.033
## Topic 7 [&#39;say&#39;, &#39;question&#39;, &#39;talk&#39;, &#39;year&#39;, &#39;bite&#39;, &#39;ask&#39;] Probability: 0.032
## Topic 8 [&#39;say&#39;, &#39;man&#39;, &#39;come&#39;, &#39;shit&#39;, &#39;right&#39;, &#39;make&#39;] Probability: 0.044
## Topic 9 [&#39;year&#39;, &#39;time&#39;, &#39;day&#39;, &#39;come&#39;, &#39;play&#39;, &#39;two&#39;] Probability: 0.043
## Topic 10 [&#39;record&#39;, &#39;label&#39;, &#39;money&#39;, &#39;make&#39;, &#39;sell&#39;, &#39;buy&#39;] Probability: 0.269
## Topic 11 [&#39;music&#39;, &#39;listen&#39;, &#39;people&#39;, &#39;dance&#39;, &#39;play&#39;, &#39;think&#39;] Probability: 0.039
## Topic 12 [&#39;play&#39;, &#39;band&#39;, &#39;music&#39;, &#39;guitar&#39;, &#39;player&#39;, &#39;musician&#39;] Probability: 0.018
## Topic 13 [&#39;people&#39;, &#39;think&#39;, &#39;music&#39;, &#39;make&#39;, &#39;good&#39;, &#39;way&#39;] Probability: 0.347
## Topic 14 [&#39;sound&#39;, &#39;record&#39;, &#39;tape&#39;, &#39;mix&#39;, &#39;use&#39;, &#39;good&#39;] Probability: 0.024</code></pre>
<p>In the case of Young Guru, the hip-hop engineer, we see topics 13 and 10 as most probable which makes sense as 13 seems to relate to theoretical ideas about working in music and 10 to the business side of music, which are definitely recurring topics in his conversation. On the other side, his least probable topic is the one relating to music and moving images, which makes sense.</p>
<p>In the Python version of the corpus, Guru’s actual work as an engineer within hip-hop is better captured with topics 1 and 7 as the most probable.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb139-1"><a href="topic-modeling.html#cb139-1" aria-hidden="true" tabindex="-1"></a>lecture_titles_py <span class="op">=</span> rbma_python_df.index.tolist()</span>
<span id="cb139-2"><a href="topic-modeling.html#cb139-2" aria-hidden="true" tabindex="-1"></a>lecture_to_check_py <span class="op">=</span> <span class="st">&quot;young guru &quot;</span></span>
<span id="cb139-3"><a href="topic-modeling.html#cb139-3" aria-hidden="true" tabindex="-1"></a>lecture_number_py <span class="op">=</span> lecture_titles_py.index(lecture_to_check_py)</span>
<span id="cb139-4"><a href="topic-modeling.html#cb139-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> topic_dist_py():</span>
<span id="cb139-5"><a href="topic-modeling.html#cb139-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Topic Distributions for </span><span class="sc">{</span>lecture_titles_py[lecture_number_py]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb139-6"><a href="topic-modeling.html#cb139-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> topic_number, (topic, topic_distribution) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(topics_py, topic_distributions_py[lecture_number_py])):</span>
<span id="cb139-7"><a href="topic-modeling.html#cb139-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Topic </span><span class="sc">{</span>topic_number<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>topic[:<span class="dv">6</span>]<span class="sc">}</span><span class="ss"> Probability: </span><span class="sc">{</span><span class="bu">round</span>(topic_distribution, <span class="dv">3</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb139-8"><a href="topic-modeling.html#cb139-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-9"><a href="topic-modeling.html#cb139-9" aria-hidden="true" tabindex="-1"></a>topic_dist_py()</span></code></pre></div>
<pre><code>## Topic Distributions for young guru 
## Topic 0 [&#39;sound&#39;, &#39;one&#39;, &#39;actually&#39;, &#39;would&#39;, &#39;use&#39;, &#39;master&#39;] Probability: 0.012
## Topic 1 [&#39;record&#39;, &#39;get&#39;, &#39;sound&#39;, &#39;work&#39;, &#39;make&#39;, &#39;track&#39;] Probability: 0.238
## Topic 2 [&#39;say&#39;, &#39;would&#39;, &#39;come&#39;, &#39;one&#39;, &#39;call&#39;, &#39;thought&#39;] Probability: 0.009
## Topic 3 [&#39;play&#39;, &#39;song&#39;, &#39;record&#39;, &#39;band&#39;, &#39;would&#39;, &#39;say&#39;] Probability: 0.012
## Topic 4 [&#39;music&#39;, &#39;get&#39;, &#39;people&#39;, &#39;play&#39;, &#39;track&#39;, &#39;tune&#39;] Probability: 0.0
## Topic 5 [&#39;music&#39;, &#39;song&#39;, &#39;come&#39;, &#39;call&#39;, &#39;sing&#39;, &#39;album&#39;] Probability: 0.0
## Topic 6 [&#39;music&#39;, &#39;play&#39;, &#39;club&#39;, &#39;time&#39;, &#39;record&#39;, &#39;people&#39;] Probability: 0.037
## Topic 7 [&#39;like&#39;, &#39;song&#39;, &#39;beat&#39;, &#39;music&#39;, &#39;hop&#39;, &#39;hip&#39;] Probability: 0.179
## Topic 8 [&#39;music&#39;, &#39;film&#39;, &#39;movie&#39;, &#39;one&#39;, &#39;song&#39;, &#39;game&#39;] Probability: 0.0
## Topic 9 [&#39;music&#39;, &#39;also&#39;, &#39;course&#39;, &#39;berlin&#39;, &#39;track&#39;, &#39;time&#39;] Probability: 0.0
## Topic 10 [&#39;like&#39;, &#39;really&#39;, &#39;think&#39;, &#39;know&#39;, &#39;yeah&#39;, &#39;thing&#39;] Probability: 0.161
## Topic 11 [&#39;record&#39;, &#39;get&#39;, &#39;people&#39;, &#39;music&#39;, &#39;label&#39;, &#39;one&#39;] Probability: 0.067
## Topic 12 [&#39;music&#39;, &#39;play&#39;, &#39;sound&#39;, &#39;instrument&#39;, &#39;piece&#39;, &#39;one&#39;] Probability: 0.0
## Topic 13 [&#39;get&#39;, &#39;say&#39;, &#39;know&#39;, &#39;like&#39;, &#39;come&#39;, &#39;want&#39;] Probability: 0.195
## Topic 14 [&#39;music&#39;, &#39;people&#39;, &#39;thing&#39;, &#39;think&#39;, &#39;way&#39;, &#39;make&#39;] Probability: 0.09</code></pre>
<p>Lastly, the little mallet wrapper library also offers a heatmap plot method which is a nice way to take a look at the probabilities between topics and texts. Below I show results for R and Python versions of the corpus, using a randomly generated sample of 10 texts. Overall the model seems able to correctly associate the most probable topics for each text in our random samples.</p>
<pre><code>import random
target_labels = random.sample(lecture_titles_r, 10)
little_mallet_wrapper.plot_categories_by_topics_heatmap(lecture_titles_r, topic_distributions_r, topics_r, output_directory_path + &#39;/categories_by_topics.pdf&#39;,target_labels=target_labels_r, dim= (13, 9))</code></pre>
<p><img src="images/topic_heatmap_r.png" width="100%" /><img src="images/topic_heatmap_py.png" width="100%" /></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R-topicmodels" class="csl-entry">
Grün, Bettina, and Kurt Hornik. 2021. <em>Topicmodels: Topic Models</em>. <a href="https://CRAN.R-project.org/package=topicmodels">https://CRAN.R-project.org/package=topicmodels</a>.
</div>
<div id="ref-R-stm" class="csl-entry">
Roberts, Margaret, Brandon Stewart, and Dustin Tingley. 2020. <em>Stm: Estimation of the Structural Topic Model</em>. <a href="http://www.structuraltopicmodel.com/">http://www.structuraltopicmodel.com/</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p><a href="http://mallet.cs.umass.edu/" class="uri">http://mallet.cs.umass.edu/</a><a href="topic-modeling.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p><a href="https://github.com/maria-antoniak/little-mallet-wrapper" class="uri">https://github.com/maria-antoniak/little-mallet-wrapper</a><a href="topic-modeling.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p><a href="https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/05-Topic-Modeling.html" class="uri">https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/05-Topic-Modeling.html</a><a href="topic-modeling.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p><a href="https://radimrehurek.com/gensim/index.html" class="uri">https://radimrehurek.com/gensim/index.html</a><a href="topic-modeling.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>R methods in this section are based on Cosima Meyer Cornelius Puschmann’s <a href="https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/#lda">Advancing Text Mining with R and quanteda</a><a href="topic-modeling.html#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p><a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">stm: R Package for Structural Topic Models</a><a href="topic-modeling.html#fnref23" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploratory-analysis-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/laurentfintoni/rbma-text-analysis-mining05-topic_modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
