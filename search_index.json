[["index.html", "Red Bull Music Academy Lectures: Text Analysis and Topic Modeling Part 1 Introduction 1.1 Pre-Processing Steps", " Red Bull Music Academy Lectures: Text Analysis and Topic Modeling Laurent Fintoni 2021-09-16 Part 1 Introduction Photo by Fabian Brennecke This project uses tools and techniques introduced in the Digital Text in the Humanities course1 to undertake basic text analysis and text mining, in the form of topic modeling, on a corpus made up of lectures by modern musicians and music industry professionals. In essence the project seeks to ensure a simple question: what can be learnt from a corpus of interviews about modern music history using text analysis and mining? The reasons for choosing this particular focus are simple: I’ve worked as a music journalist for the past 20 years, with a focus on modern music and in particular hip-hop and electronic music; and between 2016 and 2019 I worked as a project manager (and then moving images manager) on creating a complete text and video archive of all publicly available lectures presented as part of the Academy, an itinerant educational and promotional project funded by the drinks company which was active from 1998 to 2019. Since the closure of the Academy in 2019, I’ve become particularly interested in archiving and how new and existing theories and practices might intersect with my own interests in modern music history, which ultimately led me to the Digital Humanities and Digital Knowledge masters degree at the University of Bologna. As such I wanted to choose a data set for this project which I was already familiar with so that I could use my domain expertise to better evaluate the analysis and mining. As part of my work on the RBMA archive I’ve read and watched pretty much every lecture, and I’ve also worked extensively with some of my colleagues on various ways to integrate the knowledge within the lectures into other content we were creating. Throughout all this we also did a lot of thinking about different ways in which this knowledge could be viewed and used. Thanks to my studies I now have a better understanding of how, in the particular case of this project, techniques such as text analysis and mining can be used to achieve just that. I’d like to make one last note on the data, as it relates to its sourcing and origins and in the spirit of the “show your work” principle as detailed by Catherine D’Ignazio and Lauren Klein in Data Feminism (“7. Show Your Work” 2020). As D’Ignazio and Klein explain: “Data work is part of a larger ecology of knowledge, one that must be both sustainable and socially just. […] the network of people who contribute to data projects is vast and complex. Showing this work is an essential component of data feminism […]. An emphasis on labor opens the door to the interdisciplinary area of data production studies: taking a data visualization, model, or product and tracing it back to its material conditions and contexts, as well as to the quality and character of the work and the people required to make it. This kind of careful excavation can be undertaken in academic, journalistic, or general contexts, in all cases helping to make more clearly visible—and therefore to value—the work that data science rests upon.” Firstly the source files for this project are taken from GitHub user ewenme2 who in 2019, the month after the Academy was closed, created a project that used the lecture transcripts to generate new lectures. That project is sadly no longer online but his repository with the raw data is3 and that is what I used as the source for my project as it was easier than going through my own archive and reformatting my versions. I had helped create the data ewenme used, and in turn used his processing of this data. Secondly, the full archive of lectures which is at the heart of this project would not have been possible without the work of many other people: the staff, artists, and hosts who spent time on the couch, and away from it, preparing informative conversations; various staff and freelance journalists who edited the transcripts; anonymous transcribers contracted via services such as Rev; various event and video teams who organized and produced the lectures, most of whom either worked for Yadastar*, Red Bull, or other local contractors. This isn’t by any means a large data project but it remains nonetheless possible only because of all the work that came before it in creating one of the most unique music history archives currently in circulation. 1.1 Pre-Processing Steps As part of the Digital Text in the Humanities course we were introduced to various text analysis tools. This project focuses on the parallel use of two main software packages to do the analysis and mining, R and Python, largely because both have been introduced across various courses in the first year of the Digital Humanities and Digital Knowledge degree and because they are among the better known, and most common, for such uses. However I also did some preliminary, let’s call it pre-processing, work using other tools such as Voyant4 and Distant Reader5. Voyant is a great and easy to use tool, and many of what I’ve done in this project can be done much more easily with it, however it is more limited in how you can control the various processing functions and their outputs. I ultimately felt that I stood to learn more in the long run by doing a lot of this myself, though I did go back to Voyant a few times throughout to check I was obtaining similar results. Distant Reader is an open source software tool developed by Eric Lease Morgan at the University of Notre Dame. I was introduced to it around the same time I began this project and was curious about how it might combine with what we had learnt about both text analysis and distant reading in our course. Much like Voyant, Distant Reader combines many of the best known and most used text analysis techniques and tools into one easy to use package. As with Voyant, you simply feed it data and it then allows you to browse the results in a variety of formats, as well as export them. I used some of these exports as preliminary analysis of the corpus and used some of the insights gleaned from it to inform the cleaning processes described in the following chapters. As a last note, most of the raw data files referenced in the following notebooks are available in the project’s GitHub repo6. References "],["corpus-cleaning-in-r.html", "Part 2 Corpus Cleaning in R 2.1 Basic cleaning 2.2 Lemmatizing vs Stemming", " Part 2 Corpus Cleaning in R 2.1 Basic cleaning I used R and the tm package (Feinerer and Hornik 2020) as the first tools with which to clean the RBMA corpus. This was an iterative process, with around 5 to 10 passes before I got to a point where I felt the resulting versions of the corpus might be good enough to work with. Here are the main steps and takeaways from the process: I ran word counts on the documents to remove empty files (only one). I then manually renamed files to lecturer name only (they were originally the URL for the lecture page) and removed the introductory paragraphs from each as these created additional noise. I tried to use regular expressions at first to simplify and hasten this process but got stuck so I decided to move forward manually. Eventually, with some help from a friend, I was able to write a regex solution which I implemented with the Python workflow for a similar problem (and which we’ll see in the following notebook). I decided to create two different custom stopword lists. The core of each was based on the frequent noise words I knew existed in the corpus (such as “applause”, “audience member”, and the name of hosts) alongside some of the top words gleaned from the Distant Reader processing. The second list also includes lecturer names as those were additional noise within each lecture, repeated every time someone spoke. The source lectures were in simple text formats, encoded in UTF-8, and I ran tm’s punctuation removal method with the ASCII default which meant I ended up having to run additional custom content transformers to catch the remaining loose punctuation. After testing the cleaning process a few times, I built a Document Term Matrix and inspected the 15 top words by frequency and used this to create a second custom stopword list, this one focused on catching words that had emerged from the previous steps (such as “don’t” becoming “don”) as well as removing the irrelevant frequent words found via the DTM (such as “get” and “say”). I then also created some additional content transformers to sweep for fragments left behind by the removal of pronouns such as “ll” for “I’ll” and “d” for “I’d.” Below are cells showing the code I used for the cleaning and some relevant outputs.7 library(tm) #load library rbma_corpus&lt;- VCorpus(DirSource(&quot;FILES/rbma-lectures-master/data_no_empty_files_new_file_names_no_intro&quot;)); #create corpus as Volatile Corpus to ensure that anything I did remained confined to the R object writeLines(as.character(rbma_corpus[[1]])[0:5]); #print snippet of first doc to compare with final result ## A guy called Gerald ## Hello. Hello. Thank you for turning up to the lecture. My name is A Guy Called Gerald and this is my... He’s doing the presenting, actually. ## Torsten Schmidt ## Well, actually, you’re making the job a whole lot easier here because I just ran up the stairs and spent the last few hours looking at Excel sheets. I have to say ... This might be even more complicated from first sight, but it’s a lot more pleasant. ## A guy called Gerald rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(tolower)); #lowercase everything rbma_corpus&lt;- tm_map(rbma_corpus, removeNumbers); #remove numbers rbma_corpus&lt;- tm_map(rbma_corpus, removeWords, stopwords(&quot;en&quot;)) #remove stopwords using tm inbuilt list rbma_stopwords = read.csv(&quot;FILES/rbma_stop_words.csv&quot;, header = TRUE); #load in my first custom stopword list rbma_stopwords_vec&lt;- as.vector(rbma_stopwords$RBMA.stop.words); #vectorise it so it can be applied through tm_map rbma_stopwords_vec #check it ## [1] &quot;chal ravens&quot; &quot;hanna bächer&quot; &quot;benji b&quot; ## [4] &quot;rollie pemberton&quot; &quot;david nerattini&quot; &quot;davide nerattini&quot; ## [7] &quot;cognito&quot; &quot;teri gender bender&quot; &quot;frosty&quot; ## [10] &quot;brendan m gillen&quot; &quot;jeff “chairman” mao&quot; &quot;jeff mao&quot; ## [13] &quot;jeff chang&quot; &quot;hattie collins&quot; &quot;deepti datt&quot; ## [16] &quot;dj rekha&quot; &quot;lauren martin&quot; &quot;eothen “egon” alapatt&quot; ## [19] &quot;johnny hockin&quot; &quot;kenneth lobo&quot; &quot;davide bortot&quot; ## [22] &quot;julian brimmers&quot; &quot;anupa mistry&quot; &quot;noz&quot; ## [25] &quot;serko fu&quot; &quot;todd l. burns&quot; &quot;egon&quot; ## [28] &quot;patrick thévenin&quot; &quot;geraldine sarratia&quot; &quot;gerd janson&quot; ## [31] &quot;tony nwachukwu&quot; &quot;monk one&quot; &quot;heinz reich&quot; ## [34] &quot;nelson george&quot; &quot;fabio de luca&quot; &quot;nick dwyer&quot; ## [37] &quot;fergus murphy&quot; &quot;christine kakaire&quot; &quot;tim sweeney&quot; ## [40] &quot;duane jones&quot; &quot;dj soulscape&quot; &quot;brian reitzell&quot; ## [43] &quot;osunlade&quot; &quot;om’mas keith&quot; &quot;kimberly drew&quot; ## [46] &quot;carl wilson&quot; &quot;masaaki hara&quot; &quot;toby laing&quot; ## [49] &quot;tito del aguila&quot; &quot;erin macleod&quot; &quot;shawn reynaldo&quot; ## [52] &quot;audience member&quot; &quot;rui miguel abreu&quot; &quot;vivian host&quot; ## [55] &quot;sacha jenkins&quot; &quot;anthony obst&quot; &quot;andrew barber&quot; ## [58] &quot;susumu kunisaki&quot; &quot;calle dernulf&quot; &quot;shaheen ariefdien&quot; ## [61] &quot;patrick pulsinger&quot; &quot;adam baindridge&quot; &quot;christina lee&quot; ## [64] &quot;brian “b.dot” miller&quot; &quot;jospeh ghosn&quot; &quot;miss info&quot; ## [67] &quot;ian christie&quot; &quot;translator&quot; &quot;alvin blanco&quot; ## [70] &quot;étienne menu&quot; &quot;denis boyarinov&quot; &quot;todd burns&quot; ## [73] &quot;torsten schmidt&quot; &quot;emma warren&quot; &quot;red bull music academy&quot; ## [76] &quot;yeah&quot; &quot;like&quot; &quot;applause&quot; ## [79] &quot;laughs&quot; &quot;really&quot; &quot;rbma&quot; ## [82] &quot;kinda&quot; &quot;kind of&quot; &quot;audience member&quot; ## [85] &quot;just&quot; &quot;laughter&quot; &quot;aaron gonsher&quot; rbma_corpus&lt;- tm_map(rbma_corpus, removeWords, rbma_stopwords_vec); #remove custom sw rbma_corpus&lt;- tm_map(rbma_corpus, removePunctuation); #remove punctuation only at this step otherwise it would affect elements of the custom stopword list punctuation_clean&lt;- content_transformer(function(x, pattern) {return (gsub(pattern, &quot; &quot;, x))}); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;’&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;‘&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;“&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;”&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;–&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;…&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;—&quot;); rbma_corpus&lt;- tm_map(rbma_corpus, punctuation_clean, &quot;dâmfunk&quot;) #content transformer pattern to catch various loose punctuation marks as well as one lecturer name spelling that wouldn&#39;t stick through the stopword list, likely due to the accented a dtm_stopwords = read.csv(&quot;FILES/dtm_stopwords.csv&quot;, header = FALSE); #load a second custom stopword list based on examining the DTM dtm_stopwords_vec&lt;- as.vector(dtm_stopwords$V1); dtm_stopwords_vec ## [1] &quot;don&quot; &quot;can&quot; &quot;going&quot; &quot;get&quot; &quot;got&quot; &quot;something&quot; ## [7] &quot;lot&quot; &quot;thing&quot; &quot;get&quot; &quot;things&quot; &quot;one&quot; &quot;kind&quot; ## [13] &quot;stuff&quot; &quot;know&quot; &quot;want&quot; &quot;well&quot; &quot;didn&quot; rbma_corpus&lt;- tm_map(rbma_corpus, removeWords, dtm_stopwords_vec); #remove those new stopwords rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(function(x) gsub(x, pattern = &quot;\\\\st\\\\s&quot;, replacement = &quot;&quot;))); rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(function(x) gsub(x, pattern = &quot;\\\\ss\\\\s&quot;, replacement = &quot;&quot;))); rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(function(x) gsub(x, pattern = &quot;\\\\sre\\\\s&quot;, replacement = &quot;&quot;))); rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(function(x) gsub(x, pattern = &quot;\\\\sm\\\\s&quot;, replacement = &quot;&quot;))); rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(function(x) gsub(x, pattern = &quot;\\\\sll\\\\s&quot;, replacement = &quot;&quot;))); rbma_corpus&lt;- tm_map(rbma_corpus, content_transformer(function(x) gsub(x, pattern = &quot;\\\\sve\\\\s&quot;, replacement = &quot;&quot;))); #content transformer with regex to remove all the single/double letters that result from pronouns being removed rbma_corpus&lt;- tm_map(rbma_corpus, stripWhitespace) #finally strip whitespace writeLines(as.character(rbma_corpus[[1]])[0:5]); #print snippet of first doc to compare with initial result ## guy called gerald ## hello hello thank turning lecture name guy called gerald presenting actually ## ## actually making job whole easier ran stairs spent last hours looking excel sheets say might even complicated first sight pleasant ## guy called gerald The resulting corpus was labelled as RBMA CLEAN R V1. I then created a V2, which followed the same process but used the second stopword list, which includes all lecturer names. This version of the corpus was labelled RBMA CLEAN R V2. 2.2 Lemmatizing vs Stemming Once I had the basic cleaning done I did some stemming tests but decided that the effect of returning stems wasn’t useful for what I wanted to look at. I instead focused on lemmatizing, which would leave the lemma behind (rather than a potential nonsensical stem) and could potentially remove some additional noise such as multiple forms of verbs. Lemmatization in turn threw up some challenges, especially in trying to combine the tm and textstem (Rinker 2018) packages. One major issue was being unable to use a dictionary for the lemmatization process as the lemmatize_strings method from textstem has to be passed to a tm VCorpus object via tm_map and a content_transformer function. I tried using the built-in dictionaries, and I also built one from a DTM of the corpus, but in every case when I passed the dictionary via tm_map R would just run endlessly until I force quit. In the end the only thing that seemed to work was to simply pass lemmatize_strings as is. Below I show the Document Term Matrix summary outputs for RBMA CLEAN R V1 and its equivalent lemmatized version, which shows the difference in total unique term counts. rbma_corpus_v1&lt;- VCorpus(DirSource(&quot;FILES/RBMA_CLEAN_R_V1&quot;)); rbma_corpus_v1_lemm&lt;- VCorpus(DirSource(&quot;FILES/RBMA_CLEAN_R_LEMM_V1&quot;)); dtm_v1&lt;- DocumentTermMatrix(rbma_corpus_v1); dtm_v1_lemm&lt;- DocumentTermMatrix(rbma_corpus_v1_lemm); dtm_v1; ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 55227)&gt;&gt; ## Non-/sparse entries: 635651/25210585 ## Sparsity : 98% ## Maximal term length: 63 ## Weighting : term frequency (tf) dtm_v1_lemm ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 45314)&gt;&gt; ## Non-/sparse entries: 521915/20685037 ## Sparsity : 98% ## Maximal term length: 63 ## Weighting : term frequency (tf) After the lemmatization process I had another two versions of the corpus: R V1 LEMM and R V2 LEMM. Iterating through all these steps multiple times and repeatedly testing the results, for example by observing the outputs of specific corpus documents, was essential in figuring out how to catch as much as possible during the cleaning process in order to create a corpus that could be as useful as possible. As I’ll explain in the following notebooks, I undertook text analysis and mining with the R V2 LEMM version of the corpus but having multiple versions to compare at first was useful in understanding which choices to make in the cleaning process as well as the strength of various noise factors. References "],["corpus-cleaning-in-python.html", "Part 3 Corpus Cleaning in Python 3.1 Basic cleaning pt 2 3.2 Lemmatizing vs Stemming pt 2", " Part 3 Corpus Cleaning in Python 3.1 Basic cleaning pt 2 After cleaning the corpus with R, I attempted to replicate the process in Python both as a learning exercise and to see what might be different. For Python I focused on using the nltk library8, which seems to be equivalent in popularity to R’s tm package. Here are the main steps and takeaways from the process: Regular expressions are essential to doing ‘better’ cleaning in Python and after a lot of trial and error attempting to devise various patterns to do what I wanted I got some help from a friend to focus on a simple pattern that could strip each document of the lecturer or host’s name when it appears on its own line rather than throughout the text. I settled on this as a small win and a primary point of difference between the Python and R cleaning processes. In R, I removed lecturer/host names from the entire corpus, even when someone might be mentioned in another lecture, which could be an issue for any meaningful attempts at creating networks between people (the repeated use of names throughout each file is also a huge noise factor for any textual analysis). With this regex pattern I was able to remove the instances of names that are pure noise. Another major difference between Python and R that emerged through this comparison was in the ease of processing stopwords. R allows for the inclusion of n-grams in a stopword list, whereas in Python this needs to be done with additional code. I tried a few regex solutions but was unable to get something working to capture anything more than unigrams. This ended up being another difference between the two versions. In Python I broke down all the stopwords into unigrams. The primary issue with this is that one of the custom stopwords is “Red Bull Music Academy,” which ended up removing the word “music” from the Python version of the corpus, which as we’ll see is one of the most prominent words in the corpus. Below are cells showing the code I used for the cleaning and some relevant outputs. Note that to avoid running unnecessary code during the output of the notebooks, Python functions are shown as plain code with their outputs as images, code chunks are only used and output when they required minimal processing. library(reticulate) #import needed libraries import pandas as pd import nltk from nltk.corpus import PlaintextCorpusReader, wordnet import re import os.path from nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizer from nltk.tokenize.treebank import TreebankWordDetokenizer #set raw corpus to be cleaned rbma_corpus = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/rbma-lectures-master/data_no_empty_files_new_file_names_no_intro&#39;, &#39;.*\\.txt&#39;) #load custom stoplist as df lecturer_names_df = pd.read_csv(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/rbma_stop_words_2.csv&#39;) #set export directories export_directory = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_V1&#39; export_directory_two = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_V2&#39; export_directory_three = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V1&#39; export_directory_four = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V2&#39; export_directory_five = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V1_POS&#39; export_directory_six = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V2_POS&#39; #clean each doc in corpus of punct, standard stopwords, custom stopwords and lecturer/interviewer names (only on new lines) def filter_stop_punct(input_string): stopwords = nltk.corpus.stopwords.words(&#39;english&#39;) #nltk stopwords custom_stopwords = [&#39;red&#39;, &#39;bull&#39;, &#39;music&#39;, &#39;academy&#39;, &#39;yeah&#39;, &#39;like&#39;, &#39;applause&#39;, &#39;laughs&#39;, &#39;really&#39;, &#39;rbma&#39;, &#39;kinda&#39;, &#39;audience&#39;, &#39;member&#39;, &#39;just&#39;, &#39;can&#39;, &#39;going&#39;, &#39;get&#39;, &#39;got&#39;, &#39;something&#39;, &#39;lot&#39;, &#39;thing&#39;, &#39;get&#39;, &#39;things&#39;, &#39;one&#39;, &#39;kind&#39;, &#39;stuff&#39;, &#39;know&#39;, &#39;want&#39;, &#39;well&#39;] #list of custom stopwords lecturer_names = lecturer_names_df[&#39;RBMA stop words&#39;].tolist() #import lecturer/host names cleaning_regex = re.compile(&#39;\\n\\s*(&#39; + &#39;|&#39;.join(lecturer_names) + &#39;)\\s*\\n&#39;) #cleaning pattern for names punct_tokenizer = nltk.RegexpTokenizer(r&quot;\\w+&quot;) #catch punct at tokenizing stage newline_string = &#39;\\n&#39; + input_string #add a new line to catch first mention of lecturer/host text_lower = newline_string.lower() #lower everything text_clean = re.sub(cleaning_regex, &#39;\\n&#39;, text_lower) #remove names on their own line only text_tokenised = punct_tokenizer.tokenize(text_clean) #tokenize string text_clean = [w for w in text_tokenised if w not in stopwords] #remove stopwords text_clean_2 = [w for w in text_clean if w not in custom_stopwords] #remove custom stopwords text_clean_3 = TreebankWordDetokenizer().detokenize(text_clean_2) #detokenize to get clean string back text_clean_4 = re.sub(&#39;’|‘|–|“|”|…|—|dâm funk&#39;, &#39;&#39;, text_clean_3) # catch loose bits text_clean_5 = re.sub(r&#39;\\d+&#39;, &#39;&#39;, text_clean_4) #remove numbers text_clean_6 = re.sub(r&#39;\\ss\\s&#39;, &#39;&#39;, text_clean_5) #remove trailing s&#39;s text_final = &#39; &#39;.join(text_clean_6.split()) #strip whitespace return text_final #iterate through input corpus, clean, export to new directory def export_corpus(input_corpus): for d in input_corpus.fileids(): clean_string = filter_stop_punct(input_corpus.raw(d)) filename = d filepath = os.path.join(export_directory, filename) outfile = open(filepath, &#39;w&#39;) outfile.write(clean_string) outfile.close() return &#39;Process complete!&#39; #print the first few lines of the same lecture before and after cleaning rbma_corpus = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/rbma-lectures-master/data_no_empty_files_new_file_names_no_intro&#39;, &#39;.*\\.txt&#39;) rbma_corpus_py_clean_v1 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_V1&#39;, &#39;.*\\.txt&#39;) print(&#39;Raw version:\\n&#39;, rbma_corpus.raw(&#39;jam-and-lewis.txt&#39;)[0:200]) ## Raw version: ## Jeff Mao ## Our lecturers today are a songwriting and producing team from Minneapolis, Minnesota, that you may have heard of. They happen to be two of the best to have ever done it. So please welcome Jim print(&#39;Clean version:\\n&#39;, rbma_corpus_py_clean_v1.raw(&#39;jam-and-lewis.txt&#39;)[0:200]) ## Clean version: ## lecturers today songwriting producing team minneapolis minnesota may heard happen two best ever done please welcome jimmy jam terry lewis thank thank us yes talk talk would love us listen first reset The resulting corpus was labelled as RBMA CLEAN PY V1. I then ran the same function again but this time I did not remove custom stopwords and only removed lecturer and host names when they appeared on new lines, as I felt this might give me an interesting corpus version that still included the key word “music” and didn’t have host and lecturer names as noise but only where they are relevant. #clean each doc in corpus but no custom stopwords and lecturer and host names only on new lines def filter_names(input_string): stopwords = nltk.corpus.stopwords.words(&#39;english&#39;) #nltk stopwords lecturer_names = lecturer_names_df[&#39;RBMA stop words&#39;].tolist() #import lecturer/host names cleaning_regex = re.compile(&#39;\\n\\s*(&#39; + &#39;|&#39;.join(lecturer_names) + &#39;)\\s*\\n&#39;) #cleaning pattern for names punct_tokenizer = nltk.RegexpTokenizer(r&quot;\\w+&quot;) #catch punct at tokenizing stage newline_string = &#39;\\n&#39; + input_string #add a new line to catch first mention of lecturer/host text_lower = newline_string.lower() #lower everything text_clean = re.sub(cleaning_regex, &#39;\\n&#39;, text_lower) #remove names on their own line only text_tokenised = punct_tokenizer.tokenize(text_clean) #tokenize string text_clean = [w for w in text_tokenised if w not in stopwords] #remove stopwords text_clean_2 = TreebankWordDetokenizer().detokenize(text_clean) #detokenize to get clean string back text_clean_3 = re.sub(&#39;’|‘|–|“|”|…|—|dâm funk&#39;, &#39;&#39;, text_clean_2) # catch loose bits text_clean_4 = re.sub(r&#39;\\d+&#39;, &#39;&#39;, text_clean_3) #remove numbers text_clean_5 = re.sub(r&#39;\\ss\\s&#39;, &#39;&#39;, text_clean_4) #remove trailing s&#39;s text_final = &#39; &#39;.join(text_clean_5.split()) #strip whitespace return text_final #iterate through input corpus, clean, export to new directory def export_corpus_names(input_corpus): for d in input_corpus.fileids(): clean_string = filter_names(input_corpus.raw(d)) filename = d filepath = os.path.join(export_directory_two, filename) outfile = open(filepath, &#39;w&#39;) outfile.write(clean_string) outfile.close() return &#39;Process complete!&#39; The resulting corpus was labelled as RBMA CLEAN PY V2. #print the first few lines of the same lecture to show differences btw v1 and v2 rbma_corpus_py_clean_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_V2&#39;, &#39;.*\\.txt&#39;) print(&#39;Python V1:\\n&#39;, rbma_corpus_py_clean_v1.raw(&#39;jam-and-lewis.txt&#39;)[0:300]) ## Python V1: ## lecturers today songwriting producing team minneapolis minnesota may heard happen two best ever done please welcome jimmy jam terry lewis thank thank us yes talk talk would love us listen first reset clean ears whatever else play actually album produced janet jackson last year cut entitled broken he print(&#39;Python V2:\\n&#39;, rbma_corpus_py_clean_v2.raw(&#39;jam-and-lewis.txt&#39;)[0:300]) ## Python V2: ## lecturers today songwriting producing team minneapolis minnesota may heard happen two best ever done please welcome jimmy jam terry lewis applause thank thank us yes lot music talk lot things talk would love us listen something first reset clean ears whatever else want play something actually album 3.2 Lemmatizing vs Stemming pt 2 For the lemmatizing process in Python I opted to use the WordNetLemmatizer module which is bundled with nltk. The first few tries yielded no changes as I was calling the lemmatizer method directly on the raw strings. I eventually realised I needed to tokenize the raw strings first and then call the method on the tokens before detokenizing the result using nltk’s TreebankWordDetokenizer. The code for this is shown below. 9 lemmatizer = WordNetLemmatizer() #lemmatize and export cleaned corpus def lemma_corpus(input_corpus): for d in input_corpus.fileids(): tokenize_str = word_tokenize(input_corpus.raw(d)) lemmatized = [lemmatizer.lemmatize(w) for w in tokenize_str] detokenise_str = TreebankWordDetokenizer().detokenize(lemmatized) filename = d filepath = os.path.join(export_directory_three, filename) outfile = open(filepath, &#39;w&#39;) outfile.write(detokenise_str) outfile.close() return &#39;Process complete!&#39; #print the first few lines of the same lecture to show differences btw clean and lemmatized rbma_corpus_py_clean_lemm_v1 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V1&#39;, &#39;.*\\.txt&#39;) print(&#39;Clean version V1:\\n&#39;, rbma_corpus_py_clean_v1.raw(&#39;jam-and-lewis.txt&#39;)[0:500]) ## Clean version V1: ## lecturers today songwriting producing team minneapolis minnesota may heard happen two best ever done please welcome jimmy jam terry lewis thank thank us yes talk talk would love us listen first reset clean ears whatever else play actually album produced janet jackson last year cut entitled broken hearts heal jimmy jam terry lewis janet jackson janet jackson broken hearts heal bad huh ok popular songs album popular album successful album story behind every song curious story behind song share us print(&#39;Clean and lemmatized version V1:\\n&#39;, rbma_corpus_py_clean_lemm_v1.raw(&#39;jam-and-lewis.txt&#39;)[0:500]) ## Clean and lemmatized version V1: ## lecturer today songwriting producing team minneapolis minnesota may heard happen two best ever done please welcome jimmy jam terry lewis thank thank u yes talk talk would love u listen first reset clean ear whatever else play actually album produced janet jackson last year cut entitled broken heart heal jimmy jam terry lewis janet jackson janet jackson broken heart heal bad huh ok popular song album popular album successful album story behind every song curious story behind song share u broken h As we can see the process isn’t full proof, with words like “us” being lemmatized to “u” and verb forms like “produced” not lemmatized to their base lemma. This is a by-product of the default settings of the WordNetLemmatizer and it is possible to further refine the process by using Part Of Speech tagging and custom dictionaries to indicate which terms should be captured and how they should be tagged. Custom dictionaries are quite a bit of work so I skipped them for now but I did tweak the lemmatization function to add baseline POS tagging for adjectives, nouns, verbs, and adverbs. This helped increase the efficiency of the process a little. def get_wordnet_pos(word): &quot;&quot;&quot;Map POS tag to first character lemmatize() accepts&quot;&quot;&quot; tag = nltk.pos_tag([word])[0][1][0].upper() tag_dict = {&quot;J&quot;: wordnet.ADJ, &quot;N&quot;: wordnet.NOUN, &quot;V&quot;: wordnet.VERB, &quot;R&quot;: wordnet.ADV} return tag_dict.get(tag, wordnet.NOUN) def lemma_pos_corpus(input_corpus): for d in input_corpus.fileids(): tokenize_str = word_tokenize(input_corpus.raw(d)) lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokenize_str] detokenise_str = TreebankWordDetokenizer().detokenize(lemmatized) filename = d filepath = os.path.join(export_directory_four, filename) outfile = open(filepath, &#39;w&#39;) outfile.write(detokenise_str) outfile.close() return &#39;Process complete!&#39; #print the first few lines of the same lecture to show differences btw clean and lemmatized rbma_corpus_py_clean_lemm_v1_pos = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V1_POS&#39;, &#39;.*\\.txt&#39;) print(&#39;Lemmatized version V1:\\n&#39;, rbma_corpus_py_clean_lemm_v1.raw(&#39;jam-and-lewis.txt&#39;)[0:500]) ## Lemmatized version V1: ## lecturer today songwriting producing team minneapolis minnesota may heard happen two best ever done please welcome jimmy jam terry lewis thank thank u yes talk talk would love u listen first reset clean ear whatever else play actually album produced janet jackson last year cut entitled broken heart heal jimmy jam terry lewis janet jackson janet jackson broken heart heal bad huh ok popular song album popular album successful album story behind every song curious story behind song share u broken h print(&#39;Lemmatized with POS V1:\\n&#39;, rbma_corpus_py_clean_lemm_v1_pos.raw(&#39;jam-and-lewis.txt&#39;)[0:500]) ## Lemmatized with POS V1: ## lecturer today songwriting produce team minneapolis minnesota may heard happen two best ever do please welcome jimmy jam terry lewis thank thank u yes talk talk would love u listen first reset clean ear whatever else play actually album produce janet jackson last year cut entitle broken heart heal jimmy jam terry lewis janet jackson janet jackson broken heart heal bad huh ok popular song album popular album successful album story behind every song curious story behind song share u broken heart h We can see a slight increase in accuracy, especially with verb forms but we’re still getting issues with “us” and with “heard” not being recognised as a verb form. The second version of the Python clean corpus was also lemmatized resulting in eight versions of the original raw corpus: R clean v1 (custom stopwords) R clean v2 (custom stopwords + all lecturer and host names removed) R clean and lemmatized v1 and v2 (using textstem) Python clean v1 (custom stopwords as unigrams) Python clean v2 (no custom stopwords + lecturer and host names removed only from new lines) Python clean and lemmatized v1 and v2 (using WordNetLemmatizer and POS) Lastly, I ran a quick check on total word counts for each version of the corpus to make sure everything looked ok. Below is the code and the resulting dataframe, which I’d pickled in advance. #Set corpora roots rbma_corpus = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/rbma-lectures-master/data_no_empty_files_new_file_names_no_intro&#39;, &#39;.*\\.txt&#39;) rbma_corpus_clean_v1 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_R_V1&#39;, &#39;.*\\.txt&#39;) rbma_corpus_clean_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_R_V2&#39;, &#39;.*\\.txt&#39;) rbma_corpus_clean_lemm_v1 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_R_LEMM_V1&#39;, &#39;.*\\.txt&#39;) rbma_corpus_clean_lemm_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_R_LEMM_V2&#39;, &#39;.*\\.txt&#39;) rbma_corpus_py_clean_v1 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_V1&#39;, &#39;.*\\.txt&#39;) rbma_corpus_py_clean_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_V2&#39;, &#39;.*\\.txt&#39;) rbma_corpus_py_clean_lemm_v1 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V1_POS&#39;, &#39;.*\\.txt&#39;) rbma_corpus_py_clean_lemm_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V2_POS&#39;, &#39;.*\\.txt&#39;) #make a list of the corpora and give them pretty titles rbma_corpus_list = [rbma_corpus, rbma_corpus_clean_v1, rbma_corpus_clean_v2, rbma_corpus_clean_lemm_v1, rbma_corpus_clean_lemm_v2, rbma_corpus_py_clean_v1, rbma_corpus_py_clean_v2, rbma_corpus_py_clean_lemm_v1, rbma_corpus_py_clean_lemm_v2] title_list = [&#39;RBMA Raw&#39;, &#39;RBMA R V1&#39;, &#39;RBMA R V2&#39;, &#39;RBMA R V1 LEMM&#39;, &#39;RBMA R V2 LEMM&#39;, &#39;RBMA PY V1&#39;, &#39;RBMA PY V2&#39;, &#39;RBMA PY V1 LEMM&#39;, &#39;RBMA PY V2 LEMM&#39;] #get total length of corpora to compare def corpus_lengths(corpus_list): total_length = [len(c.words()) for c in corpus_list] title = [d for d in title_list] length_df = pd.DataFrame({&#39;Corpus&#39;: title, &#39;Length&#39;: total_length}).set_index(&#39;Corpus&#39;) return length_df corpus_lengths(rbma_corpus_list) rbma_corpus_lenghts = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/corpus_lengths.pkl&#39; rbma_corpus_lenghts = pd.read_pickle(rbma_corpus_lenghts) rbma_corpus_lenghts ## Length ## Corpus ## RBMA Raw 6520133 ## RBMA R V1 2045574 ## RBMA R V2 1963253 ## RBMA R V1 LEMM 2045696 ## RBMA R V2 LEMM 1963375 ## RBMA PY V1 1952929 ## RBMA PY V2 2248159 ## RBMA PY V1 LEMM 1952837 ## RBMA PY V2 LEMM 2248066 By counting all the words in the nltk corpus objects we see a slight increase in total words between the R clean and lemmatized versions, however the Document Term Matrix from 2.2 confirm that there has been a reduction in unique words. I’m not sure why this increase in total word count happened but assume it might be a by-product of the lemmatizing process creating additional tokens. Aside from this everything looks correct: R V2 has less words due to having removed all host and lecturer names; Python V2 has more words due to not having removed custom stopwords; and the lemmatized versions of the Python corpora have a slightly lesser total word count but a more meaningful drop in unique terms if we inspect their DTM. I then proceeded forward with three versions of the corpus: The raw version, as a benchmark R V2 LEMM, as a version with the highest amount of custom stopwords and names removed PY V2 LEMM, as a version with the lowest amount of custom stopwords and names removed Overall, the experience of iteratively cleaning the corpus across both R and Python in a variety of ways really underlined how this is a process that can quickly grow out of hand and needs to be reigned in at some point, as chasing the idea of a perfectly clean corpus can feel like an endless pursuit. https://www.nltk.org/book/↩︎ The following were used as reference for lemmatizing functions and code: Selva Prabhakaran’s Lemmatization Approaches with Examples in Python and Hafsa Jabeen’s Stemming and Lemmatization in Python↩︎ "],["exploratory-analysis-in-python.html", "Part 4 Exploratory Analysis in Python 4.1 POS Visualisation 4.2 Lexical diversity 4.3 Most Common Words", " Part 4 Exploratory Analysis in Python Following the cleaning process I worked on some exploratory analysis, using a mix of quantitative and qualitative approaches, to ensure that the data held up to various basic descriptions so that I could confidently move forward with topic modeling. For this I focused on the three corpus versions mentioned previously: the raw corpus, corpus lemmatized with R, and corpus lemmatized with Python. As with the cleaning process, the various analysis presented in this and the next notebook were refined through an iterative process and I’ve tried to keep the R and Python versions different, playing to each program’s strength rather than trying to replicate everything across both as I had done with the corpus cleaning. In Python, I transferred the ntlk corpus objects to pandas10, creating dataframes for each corpus version that contained various elements I would need for analysis and topic modeling. I then pickled them so they could be easily reused. These dataframes include raw transcript, tokens, POS, lexical diversity scores, and total types and tokens counts. library(reticulate) #import libraries import pandas as pd import nltk from nltk.corpus import PlaintextCorpusReader import re import matplotlib.pyplot as plt from wordcloud import WordCloud from IPython.display import display, display_html, HTML from nltk.probability import FreqDist from itertools import chain from yellowbrick.text.postag import postag from lexical_diversity import lex_div as ld from sklearn.feature_extraction.text import CountVectorizer from yellowbrick.text import FreqDistVisualizer #Set corpora roots rbma_corpus = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/rbma-lectures-master/data_no_empty_files_new_file_names_no_intro&#39;, &#39;.*\\.txt&#39;) rbma_corpus_clean_lemm_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_R_LEMM_V2&#39;, &#39;.*\\.txt&#39;) rbma_corpus_py_clean_lemm_v2 = PlaintextCorpusReader(&#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/RBMA_CLEAN_PY_LEMM_V2_POS&#39;, &#39;.*\\.txt&#39;) #make a list of the corpora and give them pretty titles rbma_corpus_list = [rbma_corpus, rbma_corpus_clean_lemm_v2, rbma_corpus_py_clean_lemm_v2] title_list = [&#39;RBMA Raw&#39;, &#39;RBMA R V2 LEMM&#39;, &#39;RBMA PY V2 LEMM&#39;] #create dataframes of each corpus and pickle for reuse later def corpus_to_df(input_corpus, title): lecture_name = [re.sub(&#39;-|.txt&#39;, &#39; &#39;, d) for d in input_corpus.fileids()] transcript = [input_corpus.raw(d) for d in input_corpus.fileids()] tokens = [input_corpus.words(d) for d in input_corpus.fileids()] total_tokens = [len(input_corpus.words(d)) for d in input_corpus.fileids()] types = [len(set(input_corpus.words(d))) for d in input_corpus.fileids()] pos = [nltk.pos_tag(input_corpus.words(d)) for d in input_corpus.fileids()] mtld = [ld.mtld(input_corpus.words(d)) for d in input_corpus.fileids()] hdd = [ld.hdd(input_corpus.words(d)) for d in input_corpus.fileids()] corpus_df = pd.DataFrame({&#39;Lecture&#39;: lecture_name, &#39;Transcript&#39;: transcript, &#39;Tokens&#39;: tokens, &#39;Total Tokens&#39;: total_tokens, &#39;Total Types&#39;: types, &#39;POS&#39;: pos, &#39;HDD Score&#39;: hdd, &#39;MTLD Score&#39;: mtld}).set_index(&#39;Lecture&#39;) corpus_df.to_pickle(f&#39;./PICKLES/{title}.pkl&#39;) return &#39;Process complete!&#39; #create list of variables for pickled dfs rbma_raw_pickle = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA Raw.pkl&#39; rbma_raw_df = pd.read_pickle(rbma_raw_pickle) rbma_corpus_clean_lemm_v2_pickle = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA R V2 LEMM.pkl&#39; rbma_R_df = pd.read_pickle(rbma_corpus_clean_lemm_v2_pickle) rbma_corpus_py_clean_lemm_v2_pickle = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA PY V2 LEMM.pkl&#39; rbma_python_df = pd.read_pickle(rbma_corpus_py_clean_lemm_v2_pickle) rbma_pickle_list = [rbma_raw_df, rbma_R_df, rbma_python_df] rbma_pickles_titles = [&#39;RBMA Raw&#39;, &#39;RBMA R&#39;, &#39;RBMA Python&#39;] 4.1 POS Visualisation I used the PosTagVisualizer from the Yellowbrick library11 to create bar charts that show the proportions of different parts of speech in a corpus. This visualisation is most meaningful in showing how the cleaning process impacted the corpus: the Python version has more verbs and modals, likely because the R version went through additional steps that removed modals that appeared in the top words, and both clean versions are composed primarily of nouns, verbs, adjectives, and adverbs. Oddly the adjective count increased slightly between each version (from 384378 in the raw corpus to 419007 and 433524 in the cleaned versions), which I assume might be due to some sort of misinterpretation in the POS tagging process. raw_list = [rbma_raw_df[&#39;POS&#39;].tolist()] r_list = [rbma_R_df[&#39;POS&#39;].tolist()] py_list = [rbma_python_df[&#39;POS&#39;].tolist()] fig, (ax1,ax2, ax3) = plt.subplots(1,3, figsize=(20,8)) fig.suptitle(&#39;POS Plots&#39;) postag(raw_list, ax=ax1, show=False) postag(r_list, ax=ax2, show=False) postag(py_list, ax=ax3, show=False) display_html(fig) 4.2 Lexical diversity Next I looked at the lexical diversity of corpus items, which is the measure of how many different words appear in a text. The corpus is best summarized as being made up of conversations about music history and practices, which can include some complex and abstract ideas (such as inspiration or technical practices). There is a small proportion of lectures by non-native English speakers, some of which are conducted in English and others in their native language via translators (I’d say between 10 and 15% at a quick glance). And the majority of conversations are around 60 to 90 minutes long (which affects the length of the transcript). Considering all this, I thought it might be interesting to see if some basic lexical diversity analysis confirmed some of these aspects of the corpus and hinted at anything else. I first used the basic Token-Type Ratio approach, which calculates the diversity of a text by dividing total unique words (types) by the total amount of works (tokens). However this approach is highly susceptible to length: the longer the document, the lower the chance that a new token will also be a new type, causing the TTR score to drop. As such, looking at TTR scores for the raw corpus we see the shortest texts with the highest scores and the longest one with the lowest scores. For reference the shortest text in the raw corpus has 2679 tokens, while the longest has 35551 tokens. To remedy this I used the lexical-diversity library in Python12, which allows for easy implementation of various other lexical diversity calculations including Hypergeometric Distribution D (HDD)13 and Measure of Lexical Textual Diversity (MTLD)14, both of which account for the limitations of TTR in different ways: MTLD computes how many words it takes before the TTR falls below a given threshold while HDD uses probability to evaluate the contribution of each word in the text to overall lexical diversity. I started by looking at the median values for tokens and types and the means for the different lexical diversity calculations across all three version of the corpus. #create DFs for all score types and assign variables to them #TTR scores raw_ttr = rbma_raw_df[&#39;Total Types&#39;] / rbma_raw_df[&#39;Total Tokens&#39;] * 100 raw_ttr = raw_ttr.sort_values(ascending=False) raw_ttr = raw_ttr.to_frame(&#39;TTR Score&#39;) r_ttr = rbma_R_df[&#39;Total Types&#39;] / rbma_R_df[&#39;Total Tokens&#39;] * 100 r_ttr = r_ttr.sort_values(ascending=False) r_ttr = r_ttr.to_frame(&#39;TTR Score&#39;) py_ttr = rbma_python_df[&#39;Total Types&#39;] / rbma_python_df[&#39;Total Tokens&#39;] * 100 py_ttr = py_ttr.sort_values(ascending=False) py_ttr = py_ttr.to_frame(&#39;TTR Score&#39;) ttr_scores = [raw_ttr, r_ttr, py_ttr] #HDD scores raw_hdd = rbma_raw_df[&#39;HDD Score&#39;].sort_values(ascending=False) raw_hdd = raw_hdd.to_frame() r_hdd = rbma_R_df[&#39;HDD Score&#39;].sort_values(ascending=False) r_hdd = r_hdd.to_frame() py_hdd = rbma_python_df[&#39;HDD Score&#39;].sort_values(ascending=False) py_hdd = py_hdd.to_frame() hdd_scores = [raw_hdd, r_hdd, py_hdd] #MTLD scores raw_mtld = rbma_raw_df[&#39;MTLD Score&#39;].sort_values(ascending=False) raw_mtld = raw_mtld.to_frame() r_mtld = rbma_R_df[&#39;MTLD Score&#39;].sort_values(ascending=False) r_mtld = r_mtld.to_frame() py_mtld = rbma_python_df[&#39;MTLD Score&#39;].sort_values(ascending=False) py_mtld = py_mtld.to_frame() mtld_scores = [raw_mtld, r_mtld, py_mtld] lex_div_averages = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/lex_div_averages.pkl&#39; lex_div_averages = pd.read_pickle(lex_div_averages) lex_div_averages ## Tokens Median Types Median TTR Mean HDD Mean MTLD Mean ## Corpus ## RBMA Raw 13365.5 1682.5 13.148197 0.830545 63.217668 ## RBMA R 4009.5 1114.0 28.540437 0.931667 101.318943 ## RBMA Python 4621.5 1151.0 25.687417 0.920383 92.314370 As we can see, the proportion of unique words to total words jumps from around 12% in the raw corpus to 25% in the cleaned versions, and in turn the various calculation scores increase. Next I looked at each different calculation method via scatter plot, to ensure that the data was evenly distributed. #create a figure and assign scatters for TTR score to it (repeat plot section with HDD and MTLD scores) raw_ttr[&#39;Total Tokens&#39;] = rbma_raw_df[&#39;Total Tokens&#39;] r_ttr[&#39;Total Tokens&#39;] = rbma_R_df[&#39;Total Tokens&#39;] py_ttr[&#39;Total Tokens&#39;] = rbma_python_df[&#39;Total Tokens&#39;] fig, (ax1,ax2, ax3) = plt.subplots(1,3, figsize=(20,8)) fig.suptitle(&#39;TTR Scores&#39;) raw_ttr.plot.scatter(x=&#39;Total Tokens&#39;, y=&#39;TTR Score&#39;, c=&#39;TTR Score&#39;, colormap=&#39;viridis&#39;, ax=ax1, title=&#39;RBMA Raw&#39;) r_ttr.plot.scatter(x=&#39;Total Tokens&#39;, y=&#39;TTR Score&#39;, c=&#39;TTR Score&#39;, colormap=&#39;viridis&#39;, ax=ax2, title=&#39;RBMA R&#39;) py_ttr.plot.scatter(x=&#39;Total Tokens&#39;, y=&#39;TTR Score&#39;, c=&#39;TTR Score&#39;, colormap=&#39;viridis&#39;, ax=ax3, title=&#39;RBMA Python&#39;) display_html(fig) These plots show a normal distribution, with the odd outlier at the top and bottom which are also reflected in the top / bottom 10 entries I looked at next. Most importantly the majority of texts in each version of the corpus fall within a range that matches the means and which I interpret as reflecting low to moderate levels of lexical diversity overall. We can also see how the cleaning process affects the plots, with the cleaned versions being more spread out. As a last step I looked at the top 10 and bottom 10 texts for each lexical diversity calculation. combined_list_ttr = [raw_ttr, r_ttr, py_ttr] combined_list_hdd = [raw_hdd, r_hdd, py_hdd] combined_list_mtld = [raw_mtld, r_mtld, py_mtld] #Display top 10 side by side, one row for each calculation, one column for each corpus - raw, R, Python (swap head for tail to display bottom 10) output = &quot;&quot; for df in combined_list_ttr: output += df.head(10).style.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;)._repr_html_() output += &quot;\\xa0\\xa0\\xa0&quot; display(HTML(output)) output = &quot;&quot; for df in combined_list_hdd: output += df.head(10).style.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;)._repr_html_() output += &quot;\\xa0\\xa0\\xa0&quot; display(HTML(output)) output = &quot;&quot; for df in combined_list_mtld: output += df.head(10).style.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;)._repr_html_() output += &quot;\\xa0\\xa0\\xa0&quot; display(HTML(output)) I found some interesting observations in these results: A handful of lectures conducted in native languages have high scores in the cleaned versions such as Kitshkrieg (German), Tullio de Piscopo (Italian), Yury Chernavsky (Russian), Wolfgang Voigt (German), Modeselektor (German), Peder Mannerfelt (Swedish) and Xavier Veilhan (French). My assumption is that these high scores are likely due to the transcripts being translations which created a higher quality of text. In the case of Voigt and Modeselektor, they also have lectures in the corpus held a decade or so before that were in English, and not their native German. Looking at the MTLD scores for these we see some sizeable differences, the translated transcripts score almost double, while differences in HDD scores are much less pronounced which is likely a reflection of how HDD is computed as HDD scores are overall in a tighter range than the other two as we can see from the scatter plots. The differences between English and German lectures for Voigt and Modeselektor lead me to assume that translations have a likely impact on lexical diversity. The other side of the above observation can be seen in lectures conducted in a mix of English and a native language or in English by non-native, non-fluent speakers, which show up in the bottom ranks. These include Yuzo Koshiro and Toshio Matsuura (conducted in Japanese with both host and translator), Damo Suzuki (Japanese native speaking English), Orchestra di Santa Cecilia (conducted in Italian and English by the lecturers), Arthur Verocai (conducted in a mix of broken English and Brazilian), and Bappi Lahiri (Indian native speaking English). Another interesting observation is the presence of lecturers from the world of hip-hop in the bottom 10 lists for HDD and MTLD scores, including A$AP Rocky, No ID, Mike Will Made-It, Frank Rodriguez, DJ Dahi, Oh No, and Ka. My assumption here is that there might be a difference in the range of vocabulary (unique types) used in these lectures which creates an adverse impact on the score similar to what happens with the lectures conducted in broken English/native languages. Other factors also include length, the Oh No lecture is one of the shortest in the corpus and thus might be more susceptible to the HDD and MTLD methodologies which try to correct for length, and context, the DJ Dahi lecture took place in Seoul and was partly translated into Korean by the host resulting in much shorter answers by the artist. Furthermore I know from experience that the Ka and No ID lectures, for example, are similar in terms of topics and ideas expressed as some of the lectures in the top 10 and so I suspect that the tendency of some of these artists to express themselves with a more limited vocabulary might be a key factor here and certainly something that would be interesting to look into further. The TTR scores confirm the known issue with the calculation. The shortest lectures appear up top (Reinboth, Madlib, Strobocop) while the longest are in the bottom (Bob Power, Marley Marl, Jam &amp; Lewis, Teddy Riley). In the case of Strobocop for example, which is one of the shortest lectures and conducted by a non-native English speaker in English, we see it in the top 10 in all three versions for TTR but in the bottom 10 for the HDD version of the R corpus, highlighting how that particular calculation method compensates for the limitations of TTR. Overall it seems to me as if HDD scoring might be most accurate in capturing the various nuances of this particular corpus and that further, more detailed lexical diversity analysis of the corpus could be interesting to see just how artists modulate their speech in public conversations. 4.3 Most Common Words Lastly I looked at the most common words in the corpus. I used the Frequency Distribution method built into nltk to generate lists alongside Yellowbrick’s FreqDist vizualizer15 and word clouds for additional visualization options.16 #return top 15 most common words in corpus and their frequency def most_common_words(input_corpus): mcw_corpus = [] for d in input_corpus.fileids(): fdist = FreqDist(input_corpus.words(d)) mcw_corpus.append(fdist.most_common()) final_list = [x for x in chain.from_iterable(mcw_corpus)] commondf = pd.DataFrame.from_records(final_list, columns=[&#39;Word&#39;, &#39;Frequency&#39;]) grouped = commondf.groupby(&#39;Word&#39;, sort=True).sum() grouped = grouped.sort_values(by=[&#39;Frequency&#39;], ascending=False) return grouped[0:15] #process all corpora to return top 15 most common words and frequencies def display_side_by_side(dfs:list, captions:list): output = &quot;&quot; combined = dict(zip(captions, dfs)) for caption, df in combined.items(): output += df.style.set_table_attributes(&quot;style=&#39;display:inline&#39;&quot;).set_caption(caption)._repr_html_() output += &quot;\\xa0\\xa0\\xa0&quot; display(HTML(output)) display_side_by_side([most_common_words(c) for c in rbma_corpus_list], [t for t in title_list]) Unsurprisingly, the raw corpus doesn’t tell us anything meaningful however the cleaned versions offer some insights. Combined together “music,” “think,” “know,” “record,” “make,” and “people” offer a simple summary of the central theme of the corpus, which is also a summary of what the Red Bull Music Academy as a project represented: the bringing together of people through music in order to learn, create, and celebrate. While raw frequency doesn’t equate relevance, music is the central idea of this corpus and as such I’d expect to see it in the top three most frequent terms. The verbs “think” and “know” reflect the kind of theorising I’d expect to see in a corpus of educational lectures about music, while “record” and “sound” reflect more physical aspects. Barcharts for the top 50 terms and wordclouds below further confirm these insights. #vectorize the corpus DFs and display barcharts of Frequency Distribution, top 50 words for R version (same for Python but call the other DF) vectorizer = CountVectorizer() docs = vectorizer.fit_transform(rbma_R_df.Transcript) features = vectorizer.get_feature_names() visualizer = FreqDistVisualizer(features=features, orient=&#39;h&#39;, size=(1440, 900), title=&#39;Frequency Distribution of Top 50 tokens in R Corpus&#39;) visualizer.fit(docs) display_html(visualizer.show()) #display wordcloud of corpus rbma_corpus_list = [rbma_corpus_clean_lemm_v2, rbma_corpus_py_clean_lemm_v2] title_list = [&#39;RBMA R Corpus&#39;, &#39;RBMA Python Corpus&#39;] def corpus_to_wordcloud(corpus_list): plt.figure(figsize=(20,8)) for i, c in enumerate(corpus_list): text = &quot; &quot;.join(t for t in c.words()) wordcloud = WordCloud(background_color=&quot;white&quot;).generate(text) plt.subplot(1, 2, i+1) plt.plot() plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.title(title_list[i]) plt.show() display_html(corpus_to_wordcloud(rbma_corpus_list)) https://pandas.pydata.org/↩︎ https://www.scikit-yb.org/en/latest/api/text/postag.html↩︎ https://pypi.org/project/lexical-diversity/↩︎ McCarthy and Jarvis, 2007, https://journals.sagepub.com/doi/10.1177/0265532207080767↩︎ McCarthy and Jarvis, 2010, https://link.springer.com/article/10.3758/BRM.42.2.381↩︎ https://www.scikit-yb.org/en/latest/api/text/freqdist.html↩︎ This Stack Overflow thread was used to create the side-by-side display function: https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side↩︎ "],["exploratory-analysis-in-r.html", "Part 5 Exploratory Analysis in R 5.1 Co-occurences 5.2 Term Frequency vs Inverse Document Frequency 5.3 Collocations", " Part 5 Exploratory Analysis in R For the exploratory analysis in R I focused on using the tm and quanteda (Benoit et al. 2021) packages: the former because of its simple methods for finding co-occurences and frequent terms across documents and the latter for its textstat sub-package and because it will also be the package I use for topic modeling in R. With quanteda I created Document Term Matrixes from the corpus versions, which were then used for the majority of operations. This allowed me to look at frequency in a different way than I had done in Python where I was only using the corpus items. library(tm) raw_corpus&lt;- VCorpus(DirSource(&quot;FILES/rbma-lectures-master/data_no_empty_files_new_file_names_no_intro&quot;)); r_corpus&lt;- VCorpus(DirSource(&quot;FILES/RBMA_CLEAN_R_LEMM_V2&quot;)); py_corpus&lt;- VCorpus(DirSource(&quot;FILES/RBMA_CLEAN_PY_LEMM_V2_POS&quot;)); raw_dtm&lt;- DocumentTermMatrix(raw_corpus); r_dtm&lt;- DocumentTermMatrix(r_corpus); py_dtm&lt;- DocumentTermMatrix(py_corpus) A quick glance at the resulting DTMs show high percentages of sparsity so I decided to reduce their sizes to make them easier to handle and help me focus on the most useful and meaningful terms. raw_dtm; r_dtm; py_dtm ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 124509)&gt;&gt; ## Non-/sparse entries: 1018563/57251649 ## Sparsity : 98% ## Maximal term length: 84 ## Weighting : term frequency (tf) ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 44933)&gt;&gt; ## Non-/sparse entries: 518877/20509767 ## Sparsity : 98% ## Maximal term length: 63 ## Weighting : term frequency (tf) ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 37549)&gt;&gt; ## Non-/sparse entries: 531743/17041189 ## Sparsity : 97% ## Maximal term length: 33 ## Weighting : term frequency (tf) raw_dtm&lt;- removeSparseTerms(raw_dtm, 0.5); r_dtm&lt;- removeSparseTerms(r_dtm, 0.5); py_dtm&lt;- removeSparseTerms(py_dtm, 0.5) raw_dtm; r_dtm; py_dtm ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 788)&gt;&gt; ## Non-/sparse entries: 278529/90255 ## Sparsity : 24% ## Maximal term length: 11 ## Weighting : term frequency (tf) ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 494)&gt;&gt; ## Non-/sparse entries: 172989/58203 ## Sparsity : 25% ## Maximal term length: 12 ## Weighting : term frequency (tf) ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 513)&gt;&gt; ## Non-/sparse entries: 180991/59093 ## Sparsity : 25% ## Maximal term length: 12 ## Weighting : term frequency (tf) 5.1 Co-occurences One the things I had struggled to easily do in Python was looking at co-occurences. However, this is much easier to do in R. The tm package provides the findAssocs method as a first step, which returns co-occurences within the DTM based on a specific term and correlation limit. I decided to look at associations for ten words from the term frequency list produced in Python which I consider thematically important to the corpus. They are: “music”, “think”, “people”, “know”, “record”, “sound”, “time”, “song”, “listen” and “track.” I set the correlation limit for each term to a level that would allow me to have enough results to analyze. In general that threshold was between 0.2 and 0.5 for the cleaned versions, and 0.3 to 0.5 for the raw corpus, denoting low to moderate levels of correlation. To make analyzing the data easier I turned these results into a combined data frame. I first assigned the findAssocs searches to variables, then turned those variables into data frames containing columns for terms, score, and corpus source, and finally combined the data frames into one. I used the dplyr (Wickham et al. 2021) and qdap (Rinker 2020) packages to do this. library(dplyr) library (qdap) #set associations to variables and turn into df raw_music&lt;- findAssocs(raw_dtm, &quot;music&quot;, 0.3); raw_music&lt;- list_vect2df(raw_music, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_music&lt;- findAssocs(r_dtm, &quot;music&quot;, 0.3); r_music&lt;- list_vect2df(r_music, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_music&lt;- findAssocs(py_dtm, &quot;music&quot;, 0.3); py_music&lt;- list_vect2df(py_music, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) music_assocs_df&lt;- bind_rows(raw_music, r_music, py_music, .id = &quot;corpus&quot;) #think associations raw_think&lt;- findAssocs(raw_dtm, &quot;think&quot;, 0.4); raw_think&lt;- list_vect2df(raw_think, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_think&lt;- findAssocs(r_dtm, &quot;think&quot;, 0.4); r_think&lt;- list_vect2df(r_think, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_think&lt;- findAssocs(py_dtm, &quot;think&quot;, 0.3); py_think&lt;- list_vect2df(py_think, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) think_assocs_df&lt;- bind_rows(raw_think, r_think, py_think, .id = &quot;corpus&quot;) #people association raw_people&lt;- findAssocs(raw_dtm, &quot;people&quot;, 0.5); raw_people&lt;- list_vect2df(raw_people, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_people&lt;- findAssocs(r_dtm, &quot;people&quot;, 0.4); r_people&lt;- list_vect2df(r_people, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_people&lt;- findAssocs(py_dtm, &quot;people&quot;, 0.4); py_people&lt;- list_vect2df(py_people, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) people_assocs_df&lt;- bind_rows(raw_people, r_people, py_people, .id = &quot;corpus&quot;) #know association raw_know&lt;- findAssocs(raw_dtm, &quot;know&quot;, 0.5); raw_know&lt;- list_vect2df(raw_know, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_know&lt;- findAssocs(r_dtm, &quot;know&quot;, 0.5); r_know&lt;- list_vect2df(r_know, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_know&lt;- findAssocs(py_dtm, &quot;know&quot;, 0.5); py_know&lt;- list_vect2df(py_know, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) know_assocs_df&lt;- bind_rows(raw_know, r_know, py_know, .id = &quot;corpus&quot;) #record association raw_record&lt;- findAssocs(raw_dtm, &quot;record&quot;, 0.4); raw_record&lt;- list_vect2df(raw_record, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_record&lt;- findAssocs(r_dtm, &quot;record&quot;, 0.4); r_record&lt;- list_vect2df(r_record, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_record&lt;- findAssocs(py_dtm, &quot;record&quot;, 0.4); py_record&lt;- list_vect2df(py_record, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) record_assocs_df&lt;- bind_rows(raw_record, r_record, py_record, .id = &quot;corpus&quot;) #sound association raw_sound&lt;- findAssocs(raw_dtm, &quot;sound&quot;, 0.3); raw_sound&lt;- list_vect2df(raw_sound, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_sound&lt;- findAssocs(r_dtm, &quot;sound&quot;, 0.3); r_sound&lt;- list_vect2df(r_sound, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_sound&lt;- findAssocs(py_dtm, &quot;sound&quot;, 0.3); py_sound&lt;- list_vect2df(py_sound, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) sound_assocs_df&lt;- bind_rows(raw_sound, r_sound, py_sound, .id = &quot;corpus&quot;) #time association raw_time&lt;- findAssocs(raw_dtm, &quot;time&quot;, 0.5); raw_time&lt;- list_vect2df(raw_time, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_time&lt;- findAssocs(r_dtm, &quot;time&quot;, 0.5); r_time&lt;- list_vect2df(r_time, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_time&lt;- findAssocs(py_dtm, &quot;time&quot;, 0.5); py_time&lt;- list_vect2df(py_time, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) time_assocs_df&lt;- bind_rows(raw_time, r_time, py_time, .id = &quot;corpus&quot;) #song association raw_song&lt;- findAssocs(raw_dtm, &quot;song&quot;, 0.4); raw_song&lt;- list_vect2df(raw_song, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_song&lt;- findAssocs(r_dtm, &quot;song&quot;, 0.4); r_song&lt;- list_vect2df(r_song, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_song&lt;- findAssocs(py_dtm, &quot;song&quot;, 0.4); py_song&lt;- list_vect2df(py_song, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) song_assocs_df&lt;- bind_rows(raw_song, r_song, py_song, .id = &quot;corpus&quot;) #listen association raw_listen&lt;- findAssocs(raw_dtm, &quot;listen&quot;, 0.3); raw_listen&lt;- list_vect2df(raw_listen, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_listen&lt;- findAssocs(r_dtm, &quot;listen&quot;, 0.3); r_listen&lt;- list_vect2df(r_listen, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_listen&lt;- findAssocs(py_dtm, &quot;listen&quot;, 0.3); py_listen&lt;- list_vect2df(py_listen, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) listen_assocs_df&lt;- bind_rows(raw_listen, r_listen, py_listen, .id = &quot;corpus&quot;) #track association raw_track&lt;- findAssocs(raw_dtm, &quot;track&quot;, 0.2); raw_track&lt;- list_vect2df(raw_track, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); r_track&lt;- findAssocs(r_dtm, &quot;track&quot;, 0.2); r_track&lt;- list_vect2df(r_track, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;); py_track&lt;- findAssocs(py_dtm, &quot;track&quot;, 0.2); py_track&lt;- list_vect2df(py_track, col1 = &quot;term&quot;, col2 = &quot;association&quot;, col3 = &quot;score&quot;) track_assocs_df&lt;- bind_rows(raw_track, r_track, py_track, .id = &quot;corpus&quot;) assocs_df&lt;- bind_rows(music_assocs_df, track_assocs_df, listen_assocs_df, song_assocs_df, time_assocs_df, sound_assocs_df, record_assocs_df, know_assocs_df, people_assocs_df, think_assocs_df) head(assocs_df, 10) ## corpus term association score ## 1 1 music way 0.30 ## 2 1 music and 0.31 ## 3 1 music think 0.31 ## 4 1 music when 0.31 ## 5 1 music which 0.31 ## 6 1 music with 0.31 ## 7 1 music also 0.32 ## 8 1 music other 0.32 ## 9 1 music that 0.32 ## 10 1 music the 0.32 (Due to how Bookdown renders R notebooks I’m only displaying the heads of long dataframes and tables but the analysis refers to the entire data) Looking at these results, we see some expected combinations for the corpus’ central thematic term of “music”: “dance music,” “find music,” “world music,” “making/make music,” “interest(ing) music/interest(ed in) music,” “music culture,” “listen(ing to) music,” “music influence,” and “play music.” Co-occurences for “record” and “sound” primarily reflect their usage as a noun or adjective rather than a verb: “make/buy/want (a) record,” “different sound,” “hit record,” “sound system,” “change (a/the) sound,” “sound effect,” “use sound,” “record(ing) studio.” Both “think” and “people” returned some obvious results such as “people make/think” though the more interesting associations were with verbs such as “feel,” “want,” “try,” “see,” and “talk” which are all relevant to the creative process. The results for “know,” “listen” and “time” were the least revealing and I’d argue these terms have more potential in the results for our other terms, such as “listen (to a) record/music.” Lastly, “song” and “track” have a degree of interchangeability in everyday usage within music discourse and some of this is reflected in the results. “Take,” “put,” “hear,” “make,” “good,” “big,” “studio,” and “producer” appear as co-occurences for both terms. The differences between co-occurences for the two terms are equally interesting with “mix,” “dance,” and “vocal” appearing only with “track” while “beat,” “crazy,” and “love” appear only with “song.” In both cases some of these can feel a little counter-intuitive: one might expect “beat” to be more easily associated with “track” or “vocal” with “song.” Seeing as these two terms appeared to have the most interplay, I decided to visualize their shared correlations in the cleaned versions via a plot, which shows “good”, “big”, and “start” as the co-occurences with the closest correlation threshold between both terms.17 corr1&lt;- findAssocs(r_dtm, &quot;song&quot;, 0.3)[[1]]; corr2&lt;- findAssocs(r_dtm, &quot;track&quot;, 0.2)[[1]] corr1&lt;- cbind(read.table(text = names(corr1), stringsAsFactors = FALSE), corr1); corr2 &lt;- cbind(read.table(text = names(corr2), stringsAsFactors = FALSE), corr2) library(dplyr) #join and arrange the two correlation results two_terms_corrs &lt;- full_join(corr1, corr2) library(tidyr) two_terms_corrs_gathered &lt;- gather(two_terms_corrs, term, correlation, corr1:corr2) two_terms_corrs_gathered$term &lt;- ifelse(two_terms_corrs_gathered$term == &quot;corr1&quot;, &quot;song&quot;, &quot;track&quot;) #create a version with only joint correlations two_terms_corrs_gathered_trim&lt;- two_terms_corrs_gathered[c(58, 163, 61, 166, 21, 126, 22, 127, 31, 136, 16, 121, 47, 152, 32, 137, 13, 118, 14, 119, 57, 162),] require(ggplot2) ggplot(two_terms_corrs_gathered_trim, aes(x = V1, y = correlation, colour = term ) ) + geom_point(size = 3) + xlab(&quot;Terms&quot;) + ylab(paste0(&quot;Correlation with the terms &quot;, &quot;\\&quot;&quot;, &quot;song&quot;, &quot;\\&quot;&quot;, &quot; and &quot;, &quot;\\&quot;&quot;, &quot;track&quot;, &quot;\\&quot;&quot;)) + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) Next I used another tm method, findMostFreqTerms, which returns a list of lists of the most frequent terms for each document in the corpus. I set the limit to 5 and then bound these lists into a single data frame showing each list as a row for easy analysis. Below we can see the top 5 and bottom 5 results for the R version of the corpus. n = 5 r_most_freq&lt;- findMostFreqTerms(r_dtm, n = n) r_most_freq&lt;- do.call(rbind, lapply(r_most_freq, function(x) { x &lt;- names(x);length(x)&lt;-n;x })) head(r_most_freq) ## [,1] [,2] [,3] [,4] [,5] ## a-guy-called-gerald.txt &quot;actually&quot; &quot;call&quot; &quot;guy&quot; &quot;people&quot; &quot;time&quot; ## addison-groove.txt &quot;play&quot; &quot;make&quot; &quot;music&quot; &quot;track&quot; &quot;people&quot; ## aisha-devi.txt &quot;music&quot; &quot;actually&quot; &quot;voice&quot; &quot;will&quot; &quot;think&quot; ## alec-empire.txt &quot;people&quot; &quot;think&quot; &quot;music&quot; &quot;time&quot; &quot;maybe&quot; ## alex-barck.txt &quot;music&quot; &quot;record&quot; &quot;people&quot; &quot;think&quot; &quot;play&quot; ## alex-rosner.txt &quot;sound&quot; &quot;room&quot; &quot;good&quot; &quot;make&quot; &quot;put&quot; tail(r_most_freq) ## [,1] [,2] [,3] [,4] [,5] ## wu-tang-clan.txt &quot;shit&quot; &quot;come&quot; &quot;time&quot; &quot;say&quot; &quot;right&quot; ## xavier-veilhan.txt &quot;work&quot; &quot;music&quot; &quot;art&quot; &quot;people&quot; &quot;time&quot; ## yoshimio.txt &quot;play&quot; &quot;right&quot; &quot;think&quot; &quot;record&quot; &quot;music&quot; ## young-guru.txt &quot;record&quot; &quot;people&quot; &quot;music&quot; &quot;make&quot; &quot;come&quot; ## yury-chernavsky.txt &quot;play&quot; &quot;music&quot; &quot;people&quot; &quot;yes&quot; &quot;first&quot; ## yuzo-koshiro.txt &quot;music&quot; &quot;time&quot; &quot;make&quot; &quot;video&quot; &quot;use&quot; A look at these results shows many of the corpus’ most frequent terms I’ve already highlighted alongside some keywords that are more specific to each document: Yuzo Koshiro’s lecture features the term “video” no doubt in relation to his work in video games; the Wu-Tang Clan likes to swear and confirm each other’s statements; Xavier Veilhan, one of the few non-musicians in the corpus, talks about “art” rather than “music”; and renowned sound system engineer Alex Rosner focused his talk on the importance of sound to physical spaces which seems clearly represented by his top 5 most frequent terms. n = 5 py_most_freq&lt;- findMostFreqTerms(py_dtm, n = n) py_most_freq&lt;- do.call(rbind, lapply(py_most_freq, function(x) { x &lt;- names(x);length(x)&lt;-n;x })) head(py_most_freq) ## [,1] [,2] [,3] [,4] [,5] ## a-guy-called-gerald.txt &quot;like&quot; &quot;actually&quot; &quot;get&quot; &quot;would&quot; &quot;thing&quot; ## addison-groove.txt &quot;make&quot; &quot;music&quot; &quot;track&quot; &quot;like&quot; &quot;get&quot; ## aisha-devi.txt &quot;music&quot; &quot;actually&quot; &quot;like&quot; &quot;voice&quot; &quot;think&quot; ## alec-empire.txt &quot;like&quot; &quot;people&quot; &quot;think&quot; &quot;music&quot; &quot;stuff&quot; ## alex-barck.txt &quot;music&quot; &quot;like&quot; &quot;record&quot; &quot;people&quot; &quot;really&quot; ## alex-rosner.txt &quot;sound&quot; &quot;room&quot; &quot;get&quot; &quot;put&quot; &quot;make&quot; tail(py_most_freq) ## [,1] [,2] [,3] [,4] [,5] ## wu-tang-clan.txt &quot;like&quot; &quot;get&quot; &quot;shit&quot; &quot;know&quot; &quot;come&quot; ## xavier-veilhan.txt &quot;work&quot; &quot;music&quot; &quot;like&quot; &quot;art&quot; &quot;thing&quot; ## yoshimio.txt &quot;like&quot; &quot;really&quot; &quot;play&quot; &quot;right&quot; &quot;record&quot; ## young-guru.txt &quot;like&quot; &quot;get&quot; &quot;thing&quot; &quot;record&quot; &quot;people&quot; ## yury-chernavsky.txt &quot;music&quot; &quot;like&quot; &quot;well&quot; &quot;people&quot; &quot;yes&quot; ## yuzo-koshiro.txt &quot;music&quot; &quot;game&quot; &quot;time&quot; &quot;like&quot; &quot;make&quot; As expected the Python version has a little more noise from not having the custom stopwords removed though we can also see a similar pattern with one or two lecture-specific keyword popping up in the top 5. 5.2 Term Frequency vs Inverse Document Frequency So far all the operations were conducted with DTMs created using term frequency as the weighting option, meaning that a word’s rank was based on its frequency across all documents: every time the word appears it gets a count and thus increases in rank. However, term frequency does not equate relevance as shown by the most frequent list of the raw corpus in 4.3. While term frequency is clearly useful to bring the main themes of a corpus to the fore as we’ve seen it does not necessarily tell us anything about their importance. I know music is central to this corpus, but I also know that music is a broad umbrella and that different types of musicians and practices are represented in the corpus. To try and get at this I decided to look at the DTMs using the Inverse Document Frequency approach (tf-idf) whereby words are weighted based on term frequency as well as relative importance to a document. The expectation then is that the results will now show lists of terms that are more specific to each documents as well as bring forward sub-themes across the corpus rather than the overall themes we’ve already seen. First I set the DTMs to variables and sparsed them as I’d done with the term frequency versions. raw_dtm_idf&lt;- DocumentTermMatrix(raw_corpus, control = list(weighting = weightTfIdf)); r_dtm_idf&lt;- DocumentTermMatrix(r_corpus, control = list(weighting = weightTfIdf)); py_dtm_idf&lt;- DocumentTermMatrix(py_corpus, control = list(weighting = weightTfIdf)) raw_dtm_idf&lt;- removeSparseTerms(raw_dtm_idf, 0.5); r_dtm_idf&lt;- removeSparseTerms(r_dtm_idf, 0.5); py_dtm_idf&lt;- removeSparseTerms(py_dtm_idf, 0.5) raw_dtm_idf; r_dtm_idf; py_dtm_idf ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 750)&gt;&gt; ## Non-/sparse entries: 260745/90255 ## Sparsity : 26% ## Maximal term length: 11 ## Weighting : term frequency - inverse document frequency (normalized) (tf-idf) ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 482)&gt;&gt; ## Non-/sparse entries: 167373/58203 ## Sparsity : 26% ## Maximal term length: 12 ## Weighting : term frequency - inverse document frequency (normalized) (tf-idf) ## &lt;&lt;DocumentTermMatrix (documents: 468, terms: 493)&gt;&gt; ## Non-/sparse entries: 171631/59093 ## Sparsity : 26% ## Maximal term length: 12 ## Weighting : term frequency - inverse document frequency (normalized) (tf-idf) Next I look at the top terms in each new DTM. raw_idf_head&lt;- head(data.frame(sort(colSums(as.matrix(raw_dtm_idf)), decreasing=TRUE)), 10); colnames(raw_idf_head)[1]&lt;- &quot;frequency&quot; r_idf_head&lt;- head(data.frame(sort(colSums(as.matrix(r_dtm_idf)), decreasing=TRUE)), 10); colnames(r_idf_head)[1]&lt;- &quot;frequency&quot; py_idf_head&lt;- head(data.frame(sort(colSums(as.matrix(py_dtm_idf)), decreasing=TRUE)), 10); colnames(py_idf_head)[1]&lt;- &quot;frequency&quot; raw_idf_head; r_idf_head; py_idf_head ## frequency ## gonna 0.1709571 ## hip-hop 0.1502452 ## black 0.1406327 ## she 0.1355853 ## jazz 0.1327173 ## drum 0.1318957 ## electronic 0.1309645 ## yeah. 0.1299675 ## song 0.1274526 ## [laughs] 0.1249164 ## frequency ## shit 0.4548952 ## hiphop 0.3880062 ## remix 0.3873030 ## band 0.3457131 ## master 0.3359250 ## song 0.3236251 ## london 0.3177508 ## tune 0.3163744 ## sing 0.3117608 ## video 0.3096802 ## frequency ## shit 0.3948568 ## gonna 0.3287453 ## band 0.3141501 ## hip 0.3060792 ## master 0.2910337 ## tune 0.2798423 ## jazz 0.2793999 ## laughter 0.2784546 ## london 0.2778346 ## video 0.2733863 As expected we can now see a different set of terms, with some of the sub-themes I’d hoped to see: “hip-hop,” “jazz,” and “electronic” are potentially important genres, which I know to be the case based on my knowledge of the corpus and something that could be worth investigating further by adding metadata information to the documents about the primary genres each lecture features; “remix” and “master” are two creative practices that show up likely due to the fact there is at least one lecture with a mastering engineer every year and because both remixes and mastering are part and parcel of how many DJs and producers work; “song,” and its alternative “tune” (in the Python version), is the only term to still appear from the ten I’d selected as central to the themes based on the most common words; “shit” appears to be the preferred swear word; and “London” might be an important city to our corpus. Next I looked at the most frequent terms for each document again, comparing tf and tf-idf. The tf version of the raw corpus tells us nothing meaningful, as expected, however the tf-idf version helps cut through some of the noise to bring themes for each document to the fore. Overall the results for the raw version remain noisy. n = 5 raw_most_freq_idf&lt;- findMostFreqTerms(raw_dtm_idf, n = n) raw_most_freq_idf&lt;- do.call(rbind, lapply(raw_most_freq_idf, function(x) { x &lt;- names(x);length(x)&lt;-n;x })) raw_most_freq&lt;- findMostFreqTerms(raw_dtm, n = n) raw_most_freq&lt;- do.call(rbind, lapply(raw_most_freq, function(x) { x &lt;- names(x);length(x)&lt;-n;x })) head(raw_most_freq_idf) ## [,1] [,2] [,3] [,4] [,5] ## a-guy-called-gerald.txt &quot;anyway,&quot; &quot;guy&quot; &quot;basically,&quot; &quot;basically&quot; &quot;you’d&quot; ## addison-groove.txt &quot;gonna&quot; &quot;drum&quot; &quot;track&quot; &quot;track.&quot; &quot;track,&quot; ## aisha-devi.txt &quot;voice&quot; &quot;space&quot; &quot;pop&quot; &quot;produce&quot; &quot;course&quot; ## alec-empire.txt &quot;“ok,&quot; &quot;go,&quot; &quot;example,&quot; &quot;felt&quot; &quot;“this&quot; ## alex-barck.txt &quot;jazz&quot; &quot;hip-hop&quot; &quot;less&quot; &quot;label&quot; &quot;label,&quot; ## alex-rosner.txt &quot;bass&quot; &quot;room&quot; &quot;high&quot; &quot;eight&quot; &quot;middle&quot; tail(raw_most_freq_idf) ## [,1] [,2] [,3] [,4] [,5] ## wu-tang-clan.txt &quot;gonna&quot; &quot;felt&quot; &quot;style&quot; &quot;like,&quot; &quot;lost&quot; ## xavier-veilhan.txt &quot;art&quot; &quot;she&quot; &quot;interested&quot; &quot;rather&quot; &quot;her&quot; ## yoshimio.txt &quot;free&quot; &quot;recording&quot; &quot;drums&quot; &quot;right?&quot; &quot;applause)&quot; ## young-guru.txt &quot;gonna&quot; &quot;hip-hop&quot; &quot;young&quot; &quot;york&quot; &quot;“ok,&quot; ## yury-chernavsky.txt &quot;created&quot; &quot;jazz&quot; &quot;sing&quot; &quot;yes,&quot; &quot;such&quot; ## yuzo-koshiro.txt &quot;space&quot; &quot;create&quot; &quot;created&quot; &quot;so,&quot; &quot;released&quot; The differences between tf and tf-idf become much clearer with the cleaned versions. r_most_freq_idf&lt;- findMostFreqTerms(r_dtm_idf, n = n) r_most_freq_idf&lt;- do.call(rbind, lapply(r_most_freq_idf, function(x) { x &lt;- names(x);length(x)&lt;-n;x })) head(r_most_freq_idf) ## [,1] [,2] [,3] [,4] [,5] ## a-guy-called-gerald.txt &quot;machine&quot; &quot;basically&quot; &quot;anyway&quot; &quot;state&quot; &quot;push&quot; ## addison-groove.txt &quot;drum&quot; &quot;exist&quot; &quot;london&quot; &quot;machine&quot; &quot;track&quot; ## aisha-devi.txt &quot;voice&quot; &quot;space&quot; &quot;sing&quot; &quot;pop&quot; &quot;connect&quot; ## alec-empire.txt &quot;video&quot; &quot;hey&quot; &quot;weird&quot; &quot;american&quot; &quot;boy&quot; ## alex-barck.txt &quot;tune&quot; &quot;jazz&quot; &quot;hiphop&quot; &quot;remix&quot; &quot;sample&quot; ## alex-rosner.txt &quot;system&quot; &quot;room&quot; &quot;ear&quot; &quot;level&quot; &quot;bass&quot; tail(r_most_freq_idf) ## [,1] [,2] [,3] [,4] [,5] ## wu-tang-clan.txt &quot;shit&quot; &quot;brother&quot; &quot;street&quot; &quot;family&quot; &quot;respect&quot; ## xavier-veilhan.txt &quot;art&quot; &quot;relationship&quot; &quot;form&quot; &quot;choose&quot; &quot;rather&quot; ## yoshimio.txt &quot;vocal&quot; &quot;free&quot; &quot;sing&quot; &quot;drum&quot; &quot;instrument&quot; ## young-guru.txt &quot;engineer&quot; &quot;hiphop&quot; &quot;york&quot; &quot;session&quot; &quot;genre&quot; ## yury-chernavsky.txt &quot;jazz&quot; &quot;sing&quot; &quot;boy&quot; &quot;musician&quot; &quot;country&quot; ## yuzo-koshiro.txt &quot;video&quot; &quot;program&quot; &quot;company&quot; &quot;street&quot; &quot;create&quot; We can now see the importance of machines to lectures by A Guy Called Gerald and Addison Groove, two artists who have made drum machines and synthesizers a key part of their practice. Aïsha Devi has “voice”, “space”, and “sing” in her top 5, which are all key aspects of her practice. It becomes clearer that Alex Barck is a DJ with a penchant for jazz, hip-hop, and remixes. The Wu still swears a lot but we can also now see the importance of familial bonds and earning respect to their career. “Form” now appears in Veilhan’s top 5 alongside “art,” a refining of his position as a sculptor in a corpus full of musicians. Yoshimio’s varied career is captured in the prominence of “free” alongside instruments she plays and uses. And Young Guru and Yuri Chernavsky are now more clearly defined: the former as a hip-hop engineer and the latter as a jazz musician. Similar results can be seen in the Python version, though again the presence of the custom stopwords removed from R create additional noise. py_most_freq_idf&lt;- findMostFreqTerms(py_dtm_idf, n = n) py_most_freq_idf&lt;- do.call(rbind, lapply(py_most_freq_idf, function(x) { x &lt;- names(x);length(x)&lt;-n;x })) head(py_most_freq_idf) ## [,1] [,2] [,3] [,4] [,5] ## a-guy-called-gerald.txt &quot;machine&quot; &quot;basically&quot; &quot;anyway&quot; &quot;state&quot; &quot;push&quot; ## addison-groove.txt &quot;gonna&quot; &quot;noise&quot; &quot;drum&quot; &quot;london&quot; &quot;exist&quot; ## aisha-devi.txt &quot;voice&quot; &quot;space&quot; &quot;sing&quot; &quot;electronic&quot; &quot;pop&quot; ## alec-empire.txt &quot;noise&quot; &quot;video&quot; &quot;felt&quot; &quot;hey&quot; &quot;game&quot; ## alex-barck.txt &quot;tune&quot; &quot;jazz&quot; &quot;hip&quot; &quot;sample&quot; &quot;less&quot; ## alex-rosner.txt &quot;low&quot; &quot;system&quot; &quot;room&quot; &quot;ear&quot; &quot;red&quot; tail(py_most_freq_idf) ## [,1] [,2] [,3] [,4] [,5] ## wu-tang-clan.txt &quot;shit&quot; &quot;brother&quot; &quot;gonna&quot; &quot;game&quot; &quot;street&quot; ## xavier-veilhan.txt &quot;art&quot; &quot;relationship&quot; &quot;form&quot; &quot;rather&quot; &quot;musician&quot; ## yoshimio.txt &quot;vocal&quot; &quot;free&quot; &quot;sing&quot; &quot;drum&quot; &quot;instrument&quot; ## young-guru.txt &quot;gonna&quot; &quot;hip&quot; &quot;hop&quot; &quot;york&quot; &quot;session&quot; ## yury-chernavsky.txt &quot;jazz&quot; &quot;sing&quot; &quot;boy&quot; &quot;musician&quot; &quot;country&quot; ## yuzo-koshiro.txt &quot;game&quot; &quot;video&quot; &quot;program&quot; &quot;company&quot; &quot;street&quot; 5.3 Collocations Collocations are another type of semantic linking between words that is easier to explore in R, allowing me to look a little beyond co-occurences. To do this I used the textstat sub-package within quanteda, first setting up new corpus objects using quanteda. library(quanteda) raw_corpus_q&lt;- corpus(raw_corpus); r_corpus_q&lt;- corpus(r_corpus); py_corpus_q&lt;- corpus(py_corpus) docnames(raw_corpus_q)&lt;- docvars(raw_corpus_q, &quot;id&quot;); docnames(r_corpus_q)&lt;- docvars(r_corpus_q, &quot;id&quot;); docnames(py_corpus_q)&lt;- docvars(py_corpus_q, &quot;id&quot;) I first looked for bigrams, combinations of two words, in the cleaned versions as the raw version doesn’t tell us anything meaningful. library(quanteda.textstats) r_col&lt;- textstat_collocations(r_corpus_q, size = 2, min_count = 500) r_col&lt;- r_col[order(r_col$count, decreasing = TRUE)] head(r_col) ## collocation count count_nested length lambda z ## 1 little bite 2836 0 2 5.720247 197.90391 ## 22 new york 2209 0 2 10.698286 48.11261 ## 14 make music 1930 0 2 2.010951 81.40333 ## 2 right now 1477 0 2 3.726804 122.93437 ## 10 come back 1213 0 2 2.885605 90.75946 ## 11 go back 1211 0 2 2.796779 88.12621 py_col&lt;- textstat_collocations(py_corpus_q, size = 2, min_count = 500) py_col&lt;- py_col[order(py_col$count, decreasing = TRUE)] head(py_col) ## collocation count count_nested length lambda z ## 1 little bit 2859 0 2 5.995948 204.99702 ## 6 hip hop 2639 0 2 12.413934 107.46136 ## 32 new york 2202 0 2 10.340784 59.19616 ## 16 make music 1891 0 2 2.099993 84.36750 ## 11 feel like 1704 0 2 2.852775 99.14666 ## 10 lot people 1701 0 2 2.694276 100.58606 Just as “London” had popped up in the top 10 terms of the tf-idf versions, here we see “New York” as another potentially important location. Both of these results lead me to think that it would be interesting to use Named Entities and network analysis on locations to see what patterns might emerge. “Hip hop” shows up again (but not in the R version as the removal of the punctation turned it into one word as we can see in the tf-idf version of the DTM), as do “electronic music” and “dance music” further hinting that these are key genres/sub-themes within the overall music theme. We also see similarities with the co-occurences we analyzed above and which are more emblematic of the corpus’ overall theme: “make music/record,” “play music,” “drum machine,” “record label.” Next I looked for trigrams, three word combinations, lowering the count threshold to 50. r_col_3&lt;- textstat_collocations(r_corpus_q, size = 3, min_count = 50) r_col_3&lt;- r_col_3[order(r_col_3$count, decreasing = TRUE)] head(r_col_3) ## collocation count count_nested length lambda z ## 7 talk little bite 231 0 3 2.297298 7.483910 ## 36 new york city 171 0 3 -1.460931 -1.648082 ## 2 mike will make 149 0 3 5.936830 9.633049 ## 40 blah blah blah 149 0 3 -8.341383 -8.679746 ## 4 start make music 134 0 3 1.071956 8.417978 ## 11 us little bite 127 0 3 1.688849 4.819971 py_col_3&lt;- textstat_collocations(py_corpus_q, size = 3, min_count = 50) py_col_3&lt;- py_col_3[order(py_col_3$count, decreasing = TRUE)] head(py_col_3) ## collocation count count_nested length lambda z ## 7 talk little bit 244 0 3 2.546546 7.600976 ## 62 new york city 169 0 3 -1.079777 -1.234271 ## 75 blah blah blah 157 0 3 -8.400974 -11.166553 ## 47 go new york 146 0 3 1.853031 1.292401 ## 35 red bull music 141 0 3 4.766094 2.891240 ## 22 bull music academy 138 0 3 8.144420 5.267650 Overall trigrams are most revealing of the way interviews in this corpus are conducted: “talk (a) little bit” (which became bite in the lemmatizing process), “tell us (a) little,” “play (a) little bit” are all recurring prompts from hosts while “blah blah blah” and “yeah yeah yeah” are often used by both lecturers and hosts to simplify stories. One last thing to note is the appearance of various artist names such (A) Guy Called Gerald, (A) Tribe Called Quest, Herb Power Jr, Mario Caldato Jr, De La Soul, and Mike Will Made-It (shown as Mike Will Make) whose tri- and quadgram names are ideal targets for the collocations. References "],["topic-modeling.html", "Part 6 Topic Modeling 6.1 Topic Modeling in R 6.2 Topic Modeling in Python", " Part 6 Topic Modeling For the last step of this project I wanted to attempt some basic text mining in the form of topic modeling, to see if the results would align with the analysis and my knowledge of the corpus. As with the rest of the project I decided to undertake this in both R and Python for learning and comparison purposes. In R I used the stm (Roberts, Stewart, and Tingley 2020) and topicmodels (Grün and Hornik 2021) packages. In Python I used the MALLET Java package18 via the Little Mallet Wrapper library19, based on Melanie Walsh’s tutorial in Introduction to Cultural Analytics &amp; Python20). In every case, topic modeling was done using the Latent Dirichlet Analysis (LDA) algorithm. I also spent some time looking into the Latent Semantic Analysis (LSA) algorithm, which is possible using stm, but this proved somewhat more complicated than I had time for. With Python, I also did some tests using the gensim library21 but the tuning required to achieve useful results was more involving than MALLET. I ran the topic models on both cleaned versions as the raw version is too noisy to be meaningful. 6.1 Topic Modeling in R To start with I created Document-Feature Matrixes from the quanteda versions of the corpus, trimming out words that appear less than 7.5% and more than 90% of the time. In my first attempts I’d set the trim theshold to ignore term frequency below 2000 but that proved too drastic resulting in topics that were a lot like the main themes I discovered in the exploratory analsysis.22 library(quanteda) library(tm) r_corpus&lt;- VCorpus(DirSource(&quot;FILES/RBMA_CLEAN_R_LEMM_V2&quot;)); py_corpus&lt;- VCorpus(DirSource(&quot;FILES/RBMA_CLEAN_PY_LEMM_V2_POS&quot;)); r_corpus_q&lt;- corpus(r_corpus); py_corpus_q&lt;- corpus(py_corpus); docnames(r_corpus_q)&lt;- docvars(r_corpus_q, &quot;id&quot;); docnames(py_corpus_q)&lt;- docvars(py_corpus_q, &quot;id&quot;) r_dfm&lt;- tokens(r_corpus_q); r_dfm&lt;- dfm(r_dfm); py_dfm&lt;- tokens(py_corpus_q); py_dfm&lt;- dfm(py_dfm) r_dfm&lt;- dfm_trim(r_dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = &quot;prop&quot;); py_dfm&lt;- dfm_trim(py_dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = &quot;prop&quot;) r_dfm; py_dfm ## Document-feature matrix of: 468 documents, 2,700 features (73.89% sparse) and 7 docvars. ## features ## docs hello turn lecture present job easy run spend hour ## a-guy-called-gerald.txt 2 1 2 1 2 4 5 1 3 ## addison-groove.txt 4 6 2 0 4 0 0 6 8 ## aisha-devi.txt 1 1 1 0 0 1 1 1 6 ## alec-empire.txt 1 4 0 2 2 4 3 0 7 ## alex-barck.txt 0 1 0 0 0 11 4 0 1 ## alex-rosner.txt 0 2 2 0 4 3 0 0 2 ## features ## docs sheet ## a-guy-called-gerald.txt 1 ## addison-groove.txt 0 ## aisha-devi.txt 0 ## alec-empire.txt 0 ## alex-barck.txt 0 ## alex-rosner.txt 0 ## [ reached max_ndoc ... 462 more documents, reached max_nfeat ... 2,690 more features ] ## Document-feature matrix of: 468 documents, 2,858 features (74.00% sparse) and 7 docvars. ## features ## docs hello thank turn lecture present job easy ran spent ## a-guy-called-gerald.txt 2 5 1 2 1 2 4 1 1 ## addison-groove.txt 1 2 6 2 0 4 0 0 2 ## aisha-devi.txt 0 5 1 1 0 0 1 0 1 ## alec-empire.txt 0 0 4 0 2 2 4 1 0 ## alex-barck.txt 0 4 1 0 0 0 11 0 0 ## alex-rosner.txt 0 0 2 2 0 4 3 0 0 ## features ## docs hour ## a-guy-called-gerald.txt 3 ## addison-groove.txt 8 ## aisha-devi.txt 6 ## alec-empire.txt 7 ## alex-barck.txt 1 ## alex-rosner.txt 2 ## [ reached max_ndoc ... 462 more documents, reached max_nfeat ... 2,848 more features ] 6.1.1 Topic Modeling with topicmodels Next I set a topic count of 15, converted the DFM to the topicmodels format and then used that to calculate the LDA models. library(topicmodels) topic.count &lt;- 15 dfm2topicmodels &lt;- convert(r_dfm, to = &quot;topicmodels&quot;) lda.model &lt;- LDA(dfm2topicmodels, topic.count) #same with python corpus dfm2topicmodels_py&lt;- convert(py_dfm, to = &quot;topicmodels&quot;) lda.model_py&lt;- LDA(dfm2topicmodels_py, topic.count) I then displayed the resulting topics in a dataframe with the top 10 terms from each for ease of analysis. as.data.frame(terms(lda.model, 10)) ## Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 ## 1 musician club everybody beat label create dj man ## 2 sing dj gonna drum tune art radio shit ## 3 jazz house man sample quite experience hiphop beat ## 4 country night kid cool bass write club rap ## 5 band dance cause hiphop london person york yo ## 6 young party cool dj release moment soul fuck ## 7 man detroit story pretty uk process disco everybody ## 8 dance techno school house obviously sense tape hiphop ## 9 african mix group definitely dubstep certain buy crazy ## 10 black scene black dance drum quite sell dude ## Topic 9 Topic 10 Topic 11 Topic 12 Topic 13 Topic 14 Topic 15 ## 1 band synthesizer room sort course mix piece ## 2 sing machine write band yes master film ## 3 drum electronic sit label label engineer movie ## 4 guitar instrument producer fuck release cut write ## 5 write computer amaze pretty berlin vinyl instrument ## 6 musician build sing money band room piano ## 7 player note hit sell example bass yes ## 8 bass control turn film techno tape game ## 9 drummer basically vocal basically french cd composer ## 10 group tape god scene quite process score as.data.frame(terms(lda.model_py, 10)) ## Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 ## 1 synthesizer hop mix musician dj person bass ## 2 instrument beat master yes club woman tune ## 3 machine hip engineer sing dance thank dj ## 4 computer sample room band house write quite ## 5 note cool cut jazz party art london ## 6 drum rap tape country disco moment drum ## 7 electronic definitely bass american night voice sort ## 8 control pretty vinyl dance mix word uk ## 9 course producer cd group scene understand club ## 10 keyboard sort high young techno sing scene ## Topic 8 Topic 9 Topic 10 Topic 11 Topic 12 Topic 13 Topic 14 Topic 15 ## 1 hip film piece man label band write cause ## 2 hop movie sort shit release man piano drum ## 3 dj band create beat sell musician string dj ## 4 everybody video quite fuck money drum band hour ## 5 rap sort interested everybody techno soul drum laugh ## 6 sample scene guess yo dj guitar sing gonna ## 7 radio course space rap certain group game machine ## 8 money write experience crazy quite everybody producer detroit ## 9 beat later instrument kid company james melody moment ## 10 gonna story sense somebody berlin knew player beat We see some expected clustering: Topics 4 (R) and 7 (Python) clearly refer to mastering / engineering lectures. 5 and 9 (R) and 14 and 15 (Python) appear to lean heavily towards DJ culture, with the Python topics making more of historically important locations such as Chicago, Detroit, (New) York and London. 8 (R) and 6 (Python) instead hint at more traditional forms of music creation (band, instruments, writing, singing). 11 (R) and 3 (Python) seem to capture some of the hip-hop related vocabulary I mentioned in 4.2 but 2 and 15 (R) and 12 (Python) do a better job at summing up hip-hop practices. 14 (R) and 8 (Python) capture the essential connection between Berlin and Detroit within techno music. 7 (R) and 9 (Python) both seem to reflect on music and moving images with “film,” “video,”, and “movie,” which might be due to the presence in the corpus of lectures with movie directors and the increasingly prevalent industry practice of syncing music to images which became a more regular topic of conversation in the second decade of lectures. 3 (R) and 1 (Python) reflect electronic music practices and instruments. 13 (R) and 5 (Python) clearly seem to point towards the business sub-theme of the corpus, with references to “label”, “buy/sell,” and “release.” Another way to view similarities between topics within the same model is to plot them using a dendogram, which shows clusters of similar topics as well as their distances/dissimilarity. lda.similarity &lt;- as.data.frame(lda.model@beta) %&gt;% scale() %&gt;% dist(method = &quot;euclidean&quot;) %&gt;% hclust(method = &quot;ward.D2&quot;) par(mar = c(0, 4, 4, 2)) plot(lda.similarity, main = &quot;LDA topic similarity by features&quot;, xlab = &quot;&quot;, sub = &quot;&quot;) In the R version, we can see that topics 3 (electronic music) and 5 (DJ culture) and 13 (business) appear most distant from the rest and that there are two major clusters: in the case of topics 1, 9, 11, 2, and 15 it seems logical to assume that this is due to how they relate to hip-hop and club cultures, while the second cluster is more diverse though within that topics 7, 10, 12, and 14 seem to be closely related via more abstract themes such as art, technology, and experimentation. In the Python version, we again see the electronic music topic (1) as most distant alongside two of the hip-hop topics (2, 3), though this distance is less pronounced, and the two major clusters appear to have similar internal relations, with DJing, hip-hop, writing, and production on the left and mastering, electronic music, clubs, and machines on the right. lda.similarity &lt;- as.data.frame(lda.model_py@beta) %&gt;% scale() %&gt;% dist(method = &quot;euclidean&quot;) %&gt;% hclust(method = &quot;ward.D2&quot;) par(mar = c(0, 4, 4, 2)) plot(lda.similarity, main = &quot;LDA topic similarity by features&quot;, xlab = &quot;&quot;, sub = &quot;&quot;) Lastly we can also view the most likely topics for each document using the get_topics method from topicmodels. For example if we check which documents are likely associated with topic 3 in R (electronic music) we see lectures with pioneers of synthesis such as Bob Moog, Don Buchla, Suzanne Ciani, and Pauline Oliveros as well as Orchestra di Santa Cecilia, most likely due to the topic also including terms such as “note,” “piano,” and “instrument.” Similar results can be obtained for other well defined topics (both in R and Python) but I was also curious to look into topics that are harder to define, such as topic 12 (R) and 13 (Python) which seem to point towards abstract ideas of experimentation. In both cases we get quite a lot of results (25 in R, 44 in Python) across a range of lectures that do seem to confirm experimentation as perhaps the underlying idea of the topic. Below is the code I used to get the most likely topics and filter them for easy analysis. topics_by_doc&lt;- get_topics(lda.model) topics_by_doc&lt;- t(topics_by_doc) topics_by_doc_df&lt;- as.data.frame(topics_by_doc) topics_count&lt;- colCounts(topics_by_doc, value = 12, useNames = TRUE, na.rm = TRUE) topics_count&lt;- as.data.frame(topics_count) topics_count_2&lt;- subset(topics_count, topics_count &gt; 0) 6.1.2 Topic Modeling with stm The stm package is similar to topicmodels but allows for more diagnosis and fine tuning, including the estimation and evaluation of potential models ahead of actually working with them.23 In the context of this project, stm was the first topic modeling package I ran tests with and it offers a nice range of quick vizualisations. As with topicmodels, I first converted the dfm to the stm format before calculating the models. library(stm) dfm2stm &lt;- convert(r_dfm, to = &quot;stm&quot;) model.stm &lt;- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, verbose = FALSE) #same with python corpus dfm2stm_py &lt;- convert(py_dfm, to = &quot;stm&quot;) model.stm_py &lt;- stm(dfm2stm_py$documents, dfm2stm_py$vocab, K = topic.count, verbose = FALSE) Again, we can first view the resulting topics as a data frame. Overall the topics appear to be similar to those obtained with topicmodels but with variations in which terms make the top 10 (topic 1 in Python is the most obviously different between the two package results). as.data.frame(t(labelTopics(model.stm, n = 10)$prob)) ## V1 V2 V3 V4 V5 V6 V7 V8 ## 1 techno hiphop man synthesizer beat label sing bass ## 2 berlin sample shit machine game sell speak tune ## 3 course beat fuck build producer money black drum ## 4 detroit drum everybody electronic rap release country london ## 5 club dj gonna instrument send buy musician quite ## 6 label rap yo computer cool pay young dj ## 7 yes everybody kid note definitely sign understand uk ## 8 dj cool rap control sample company woman club ## 9 germany pretty y basically create remix art scene ## 10 german crazy crazy company hiphop quite person sort ## V9 V10 V11 V12 V13 V14 V15 ## 1 mix film band robert piece club sort ## 2 master movie sing york instrument dj band ## 3 engineer video write disco yes dance guess ## 4 room sort drum house write house write ## 5 cut scene guitar club piano party pretty ## 6 vinyl story player tony musician night quite ## 7 tape clip musician group create mix obviously ## 8 bass write bass detroit course djing cool ## 9 loud shoot hit radio example scene process ## 10 process basically producer light voice disco tour as.data.frame(t(labelTopics(model.stm_py, n = 10)$prob)) ## V1 V2 V3 V4 V5 V6 V7 ## 1 thank hip man synthesizer label beat tune ## 2 person hop shit machine release game bass ## 3 art sample fuck computer sell producer dj ## 4 write dj everybody instrument money sample drum ## 5 woman beat yo control techno definitely london ## 6 voice rap gonna note dj cool quite ## 7 moment drum rap company company create uk ## 8 sing cool kid basically club video club ## 9 word everybody money electronic quite rap scene ## 10 understand pretty knew design business pretty sort ## V8 V9 V10 V11 V12 V13 V14 V15 ## 1 band film write mix band piece dj dj ## 2 musician movie drum master sort instrument club detroit ## 3 jazz video band engineer pretty yes dance house ## 4 country sort sing room quite piano disco club ## 5 young course hit cut guess write night chicago ## 6 black scene player tape write musician party city ## 7 group story guitar vinyl tour course house techno ## 8 african clip producer bass obviously example scene dance ## 9 sing later man cd pop create mix party ## 10 africa write bass high label classical french night The summary plot function in stm makes it much easier to see how prevalent each topic is across the corpus and its overall theme. In both versions, the three most prevalent topics are all related to writing, singing, and instruments while hip-hop, DJing, and electronic music occupy the middle range and the mastering/engineering, synthesizer building, and film topics in the bottom 5. plot(model.stm, type = &quot;summary&quot;, text.cex = 0.7, main = &quot;STM topic shares for R version&quot;, xlab = &quot;Share estimation&quot;) plot(model.stm_py, type = &quot;summary&quot;, text.cex = 0.7, main = &quot;STM topic shares for Python version&quot;, xlab = &quot;Share estimation&quot;) There is also a word cloud function in stm, which is handy for delving deeper into the topics, as well as a perspective version of the plot which allows us to see similarities and differences between topics. For example, looking at the topics from the R version for hip-hop (2) and electronic music (4) we can see that “drum,” “keyboard,” “break,” and “money” are all pretty central to both topics while “sample” and “synthesizers,” as well as the genre names of “hip-hop” and “electronic” are more closely aligned with each respective topic. At a first glance this seems to align with how people talk about and practice both genres. plot(model.stm, type = &quot;perspectives&quot;, n = 30, topics = c(2, 4), main = &quot;Putting two different topics in perspective&quot;) 6.2 Topic Modeling in Python For topic modelling in Python I made use of the pickled dataframes created during the exploratory analysis. Unlike in R, the little mallet wrapper works directly with the texts rather than DFMs. library(reticulate) import pandas as pd rbma_corpus_clean_lemm_v2_pickle = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA R V2 LEMM.pkl&#39; rbma_R_df = pd.read_pickle(rbma_corpus_clean_lemm_v2_pickle) rbma_corpus_py_clean_lemm_v2_pickle = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/PICKLES/RBMA PY V2 LEMM.pkl&#39; rbma_python_df = pd.read_pickle(rbma_corpus_py_clean_lemm_v2_pickle) path_to_mallet = &#39;/Users/laurentfintoni/mallet-2.0.8/bin/mallet&#39; import little_mallet_wrapper import seaborn from pathlib import Path We check the statistics for our dataset, which align with what we saw before. little_mallet_wrapper.print_dataset_stats(rbma_R_df.Transcript) ## Number of Documents: 468 ## Mean Number of Words per Document: 4195.0 ## Vocabulary Size: 45354 little_mallet_wrapper.print_dataset_stats(rbma_python_df.Transcript) ## Number of Documents: 468 ## Mean Number of Words per Document: 4803.6 ## Vocabulary Size: 37950 We then set the number topics to 15 and created variables for the two training sets and their respective output paths. num_topics = 15 training_data = rbma_R_df.Transcript training_data_py = rbma_python_df.Transcript output_directory_path = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/Topic Model Output/RBMA R FINAL&#39; Path(f&quot;{output_directory_path}&quot;).mkdir(parents=True, exist_ok=True) path_to_training_data = f&quot;{output_directory_path}/training.txt&quot; path_to_formatted_training_data = f&quot;{output_directory_path}/mallet.training&quot; path_to_model = f&quot;{output_directory_path}/mallet.model.{str(num_topics)}&quot; path_to_topic_keys = f&quot;{output_directory_path}/mallet.topic_keys.{str(num_topics)}&quot; path_to_topic_distributions = f&quot;{output_directory_path}/mallet.topic_distributions.{str(num_topics)}&quot; #same for python version output_directory_path_py = &#39;/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 1/Q3/DIGITAL TEXT/PROJECT/Topic Model Output/RBMA Python FINAL&#39; Path(f&quot;{output_directory_path_py}&quot;).mkdir(parents=True, exist_ok=True) path_to_training_data_py = f&quot;{output_directory_path_py}/training.txt&quot; path_to_formatted_training_data_py = f&quot;{output_directory_path_py}/mallet.training&quot; path_to_model_py = f&quot;{output_directory_path_py}/mallet.model.{str(num_topics)}&quot; path_to_topic_keys_py = f&quot;{output_directory_path_py}/mallet.topic_keys.{str(num_topics)}&quot; path_to_topic_distributions_py = f&quot;{output_directory_path_py}/mallet.topic_distributions.{str(num_topics)}&quot; Finally we train the models and export the data to be used for analysis in the relevant folders. little_mallet_wrapper.quick_train_topic_model(path_to_mallet, output_directory_path, num_topics, training_data) We can then take the results and put them in a dataframe to quickly see how they differ from the R packages. topics_r = little_mallet_wrapper.load_topic_keys(path_to_topic_keys) topics_py = little_mallet_wrapper.load_topic_keys(path_to_topic_keys_py) topics_df = pd.DataFrame([i for i in topics_r]) topics_df = topics_df.transpose() topics_df topics_df_py = pd.DataFrame([i for i in topics_py]) topics_df_py = topics_df_py.transpose() topics_df_py Overall the topics appear to be pretty similar to what the R packages returned though by working on the entire corpus rather than the matrixes we see many of the previously identified frequent words - “music,” “say,” and “like” - creeping up across all topics. As a result it’s a little harder to define these topics, though having more than 10 words for each to look at helps compensate. For example in the Python version of the corpus, it looks like the mastering/engineering topic has been split across topics 0 and 1 while in the R version (topic 14) it’s most similar to what modeling in R produced. I also took a look at probable topic distribution for specific lectures. topic_distributions_r = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions) topic_distributions_py = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions_py) lecture_titles_r = rbma_R_df.index.tolist() lecture_to_check_r = &quot;young guru &quot; lecture_number_r = lecture_titles_r.index(lecture_to_check_r) def topic_dist_r(): print(f&quot;Topic Distributions for {lecture_titles_r[lecture_number_r]}&quot;) for topic_number, (topic, topic_distribution) in enumerate(zip(topics_r, topic_distributions_r[lecture_number_r])): print(f&quot;Topic {topic_number} {topic[:6]} Probability: {round(topic_distribution, 3)}&quot;) topic_dist_r() ## Topic Distributions for young guru ## Topic 0 [&#39;new&#39;, &#39;play&#39;, &#39;club&#39;, &#39;york&#39;, &#39;people&#39;, &#39;come&#39;] Probability: 0.031 ## Topic 1 [&#39;people&#39;, &#39;black&#39;, &#39;come&#39;, &#39;world&#39;, &#39;country&#39;, &#39;say&#39;] Probability: 0.019 ## Topic 2 [&#39;film&#39;, &#39;movie&#39;, &#39;music&#39;, &#39;video&#39;, &#39;see&#39;, &#39;game&#39;] Probability: 0.011 ## Topic 3 [&#39;call&#39;, &#39;record&#39;, &#39;guy&#39;, &#39;say&#39;, &#39;band&#39;, &#39;name&#39;] Probability: 0.021 ## Topic 4 [&#39;song&#39;, &#39;track&#39;, &#39;record&#39;, &#39;album&#39;, &#39;play&#39;, &#39;make&#39;] Probability: 0.046 ## Topic 5 [&#39;sound&#39;, &#39;use&#39;, &#39;drum&#39;, &#39;play&#39;, &#39;make&#39;, &#39;bass&#39;] Probability: 0.025 ## Topic 6 [&#39;sound&#39;, &#39;music&#39;, &#39;way&#39;, &#39;different&#39;, &#39;think&#39;, &#39;also&#39;] Probability: 0.033 ## Topic 7 [&#39;say&#39;, &#39;question&#39;, &#39;talk&#39;, &#39;year&#39;, &#39;bite&#39;, &#39;ask&#39;] Probability: 0.032 ## Topic 8 [&#39;say&#39;, &#39;man&#39;, &#39;come&#39;, &#39;shit&#39;, &#39;right&#39;, &#39;make&#39;] Probability: 0.044 ## Topic 9 [&#39;year&#39;, &#39;time&#39;, &#39;day&#39;, &#39;come&#39;, &#39;play&#39;, &#39;two&#39;] Probability: 0.043 ## Topic 10 [&#39;record&#39;, &#39;label&#39;, &#39;money&#39;, &#39;make&#39;, &#39;sell&#39;, &#39;buy&#39;] Probability: 0.269 ## Topic 11 [&#39;music&#39;, &#39;listen&#39;, &#39;people&#39;, &#39;dance&#39;, &#39;play&#39;, &#39;think&#39;] Probability: 0.039 ## Topic 12 [&#39;play&#39;, &#39;band&#39;, &#39;music&#39;, &#39;guitar&#39;, &#39;player&#39;, &#39;musician&#39;] Probability: 0.018 ## Topic 13 [&#39;people&#39;, &#39;think&#39;, &#39;music&#39;, &#39;make&#39;, &#39;good&#39;, &#39;way&#39;] Probability: 0.347 ## Topic 14 [&#39;sound&#39;, &#39;record&#39;, &#39;tape&#39;, &#39;mix&#39;, &#39;use&#39;, &#39;good&#39;] Probability: 0.024 In the case of Young Guru, the hip-hop engineer, we see topics 13 and 10 as most probable which makes sense as 13 seems to relate to theoretical ideas about working in music and 10 to the business side of music, which are definitely recurring topics in his conversation. On the other side, his least probable topic is the one relating to music and moving images, which makes sense. In the Python version of the corpus, Guru’s actual work as an engineer within hip-hop is better captured with topics 1 and 7 as the most probable. lecture_titles_py = rbma_python_df.index.tolist() lecture_to_check_py = &quot;young guru &quot; lecture_number_py = lecture_titles_py.index(lecture_to_check_py) def topic_dist_py(): print(f&quot;Topic Distributions for {lecture_titles_py[lecture_number_py]}&quot;) for topic_number, (topic, topic_distribution) in enumerate(zip(topics_py, topic_distributions_py[lecture_number_py])): print(f&quot;Topic {topic_number} {topic[:6]} Probability: {round(topic_distribution, 3)}&quot;) topic_dist_py() ## Topic Distributions for young guru ## Topic 0 [&#39;sound&#39;, &#39;one&#39;, &#39;actually&#39;, &#39;would&#39;, &#39;use&#39;, &#39;master&#39;] Probability: 0.012 ## Topic 1 [&#39;record&#39;, &#39;get&#39;, &#39;sound&#39;, &#39;work&#39;, &#39;make&#39;, &#39;track&#39;] Probability: 0.238 ## Topic 2 [&#39;say&#39;, &#39;would&#39;, &#39;come&#39;, &#39;one&#39;, &#39;call&#39;, &#39;thought&#39;] Probability: 0.009 ## Topic 3 [&#39;play&#39;, &#39;song&#39;, &#39;record&#39;, &#39;band&#39;, &#39;would&#39;, &#39;say&#39;] Probability: 0.012 ## Topic 4 [&#39;music&#39;, &#39;get&#39;, &#39;people&#39;, &#39;play&#39;, &#39;track&#39;, &#39;tune&#39;] Probability: 0.0 ## Topic 5 [&#39;music&#39;, &#39;song&#39;, &#39;come&#39;, &#39;call&#39;, &#39;sing&#39;, &#39;album&#39;] Probability: 0.0 ## Topic 6 [&#39;music&#39;, &#39;play&#39;, &#39;club&#39;, &#39;time&#39;, &#39;record&#39;, &#39;people&#39;] Probability: 0.037 ## Topic 7 [&#39;like&#39;, &#39;song&#39;, &#39;beat&#39;, &#39;music&#39;, &#39;hop&#39;, &#39;hip&#39;] Probability: 0.179 ## Topic 8 [&#39;music&#39;, &#39;film&#39;, &#39;movie&#39;, &#39;one&#39;, &#39;song&#39;, &#39;game&#39;] Probability: 0.0 ## Topic 9 [&#39;music&#39;, &#39;also&#39;, &#39;course&#39;, &#39;berlin&#39;, &#39;track&#39;, &#39;time&#39;] Probability: 0.0 ## Topic 10 [&#39;like&#39;, &#39;really&#39;, &#39;think&#39;, &#39;know&#39;, &#39;yeah&#39;, &#39;thing&#39;] Probability: 0.161 ## Topic 11 [&#39;record&#39;, &#39;get&#39;, &#39;people&#39;, &#39;music&#39;, &#39;label&#39;, &#39;one&#39;] Probability: 0.067 ## Topic 12 [&#39;music&#39;, &#39;play&#39;, &#39;sound&#39;, &#39;instrument&#39;, &#39;piece&#39;, &#39;one&#39;] Probability: 0.0 ## Topic 13 [&#39;get&#39;, &#39;say&#39;, &#39;know&#39;, &#39;like&#39;, &#39;come&#39;, &#39;want&#39;] Probability: 0.195 ## Topic 14 [&#39;music&#39;, &#39;people&#39;, &#39;thing&#39;, &#39;think&#39;, &#39;way&#39;, &#39;make&#39;] Probability: 0.09 Lastly, the little mallet wrapper library also offers a heatmap plot method which is a nice way to take a look at the probabilities between topics and texts. Below I show results for R and Python versions of the corpus, using a randomly generated sample of 10 texts. Overall the model seems able to correctly associate the most probable topics for each text in our random samples. import random target_labels = random.sample(lecture_titles_r, 10) little_mallet_wrapper.plot_categories_by_topics_heatmap(lecture_titles_r, topic_distributions_r, topics_r, output_directory_path + &#39;/categories_by_topics.pdf&#39;,target_labels=target_labels_r, dim= (13, 9)) References "],["conclusion.html", "Part 7 Conclusion", " Part 7 Conclusion This project set out to answer a simple question - what can be learnt from a corpus of interviews about modern music history using text analysis and mining? - and in the process I was reminded that more often than not the search for answers only leads to more questions. This has been true in my experience as a journalist and it is true of my early days as a Digital Humanist. In the end though I feel that I have accomplished what I set out to do. There is clearly plenty to be learnt from applying text analysis and mining to modern music history texts: how people talk about music publicly and privately, what vocabulary they use, and what networks might link the things they talk about. As it regards this project, my main takeaway is that the texts themselves require some more preparatory work to facilitate better analysis and machine learning. A first step is to add useful metadata to the files such as categories (for example based on genre or location) and the year that the lecture took place. These two things alone would then allow us to look at changes in the lectures over time or the different ways in which artists from specific genres talk about their practice. Such additional metadata is common in text datasets and could easily be added to the existing version of the corpus in an additional text file which can then be read by corpus packages and libraries. Alongside this, XML could be used to markup the text in more detail, for example with POS to facilitate better lemmatization during the cleaning process or to help with creating network vizualisations of locations and people mentioned in the lectures. During the process of putting this project together I spent some time looking into Named Entity Recognition, which allows you to extract entities such as locations and people from text. I ran some tests on the corpus using the spacy library in Python, for example counting the instances when a specific location was mentioned, and it’s clear to me that there is potential there for some interesting analysis and the creation of networks based on this information. As a closing note, I also believe that music journalism has potential as a field of study within the practice of distant reading. While music journalism does not have the same time span as fiction, to reference one of the most popular fields for distant reading, it does have a lot of text to work with. In the past century alone there have been thousands of music magazines and books published and in the past 30 years more digital items than can be counted have been put online. Obviously not all these are available, for various reasons starting with copyright, but in undertaking this project I have been thinking a lot about what it might look like to begin pulling together music journalism texts into various data sets that can be used for analysis. Some of this already exists, for example there is a data set of reviews taken from the popular online magazine Pitchfork24, many US music magazines such as Billboard and Vibe are available on Google Books, and various fanzine archives exist on archive.org. I’d always wanted the RBMA lecture archive to be an addition to these existing pockets of music journalism history but now I’m also thinking about what it might be like to start bringing all these things together, rather than leaving them separate, as well as how we can use things like Optical Character Recognition to bring into the digital realm important music journalism that still only exists in print. As Ted Underwood put it in Distant Horizons: Digital Evidence and Literary Change, “there will certainly be cases where quantitative evidence uncovers puzzles that still lack an explanation.”(Underwood 2019) Music journalists and writers often come across such puzzles during their work and spend years trying to figure them out. It’s becoming increasingly clear to me that the ways in which the Digital Humanities help us combine the qualitative with the quantitative can be of help to them too. Photo by Karel Chladek References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
